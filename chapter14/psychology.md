第十四章 心理学

强化学习和心理学对学习的研究关系密切。强化学习的许多基本算法都受到了心理学学习理论的启发，而强化学习算法和理论也反过来对心理学的实验设计和建立模型做出了贡献。本章的目标是讨论强化学习的概念和算法，与心理学的学习理论有着怎样的对应关系。

应当记住，本书的目的不是从细节上再现或者解释动物及人类是如何学习的。我们建立的强化学习，是从人工智能或者工程学角度探讨的理想情形，并不是心理学家研究动物和人的方式。我们置身于心理学之外的立场，使得我们避开了影响心理学史的许多悬而未决、错综复杂的争议问题，这些争议影响了当代心理学学习理论。出于同样的理由，并不是计算强化学习的每个特征都对应于某个心理学发现或理论。强化学习的未来发展有可能联系到更多动物学习的有关发现，加入这些发现在计算层面上的意义被基于更多关注。但是目前为止我们主要关注具有明确计算意义的动物学习特征，很多例子涉及到各自领域独立提出的观点之间存在的联系。我们相信探讨这些联系，可以促进我们对计算科学和心理学学习原则的理解。

我们主要介绍强化学习和学习理论的关系，这些理论介绍了老鼠、鸽子和兔子等动物如何在可控的实验室条件下学习。数以千计的实验在20世纪早期完成的，其中许多今天仍然在进行。尽管一些实验被认为和心理学议题无关，但是这些实验探测到了动物学习的细微特征，而这常常是从精确的理论问题出发而得到的。实验室实验可以控制动物的经验，并且准确观察其行为，而这两个方面在田野实验条件下很难做到。当心理学的关注点转向行为当中那些和认知更为相关的问题，比如思维和推理，动物学习实验在心理学当中的地位开始下降。然而正是从这些实验出发，得到了广泛适用于动物王国的基本学习法则，而这些法则在设计人工智能系统的时候不应当被忽视。此外，我们将看到，认知加工的某些方面和强化学习的计算角度有着天然的联系。

强化学习和心理学和行为科学的领域之间有着许多联系，这超出了本书的范围。强化学习的一些方面是关于当学习发生之后，如何选择行动，或者说如何做出决策。基于预测行动长期结果的价值函数选择行动，是许多强化学习算法的突出特征，也和心理学的决策理论有关。我们也没有探讨生态学家和行为生态学家研究的，行为的生态和进化方面的内容，比如说动物和其他动物以及物理环境如何关联，动物的行为如何有助于进化适应。优化，MDPs以及动态规划在这些领域都颇具影响，我们强调智能体和动态环境的交互作用和智能体在复杂生态系统当中的行为有关。本书没有涉及的多智能体强化学习，和社会行为有关。此外，尽管讨论较少，强化学习也不应当忽视进化角度的解释。强化学习并未提示和学习和行为的白板说有关的证据。实际上，和经验有关的工程学应用，强调知识存在于强化学习系统当中，就像是进化提供给动物的先天知识。

本章最后一节提供了一些参考文献，包括我们讨论的这些跨学科联系，以及我们尚未讨论的那些联系。我们希望本章能够鼓励读者更为深入地探索这些议题。

14.1术语

许多强化学习领域当中的术语，也包括强化学习这个词本身，来自于实验心理学家对动物学习理论的表述。但是计算机科学家/工程师和心理学家在使用这些术语时所指可能不同。我们在这里解释一些最明显的术语差异，以免引起读者混淆。

正如第1.7节提到的，心理学当中的强化学习原本指的是行为模式

心理学家一般不使用强化学习这个具体的术语。动物学习研究者一般避免使用学习机制或者类型这类表述，而只用强化来指代动物行为随经验变化的过程。但是计算机科学家和工程师则不受此限制，尽管受到动物学习研究的启发，他们对涉及人工学习系统更感兴趣。我们追随计算机科学家和工程师的习惯来使用强化学习术语。

而奖赏一般用于指动物接近并想得到的物体或者想要令其发生的事件。为了认可动物“好”的行为，或者为了让动物表现“更好”，人们会给动物奖赏。类似地，惩罚就是动物通常回避的物体或事件，或者由于“不好”的行为而遭遇的物体或招致的事件。初级强化物是动物在进化当中为了提高生存和繁殖机会，从而在神经系统当中生来就具有的构造所能带来的，例如品尝美味食物、性接触、成功逃脱危险所带来的奖赏，以及其他该物种祖先时代起就预示着能成功繁育后代的刺激或事件。次级强化物指的是预示着能够带来其他初级强化物或者次级强化物的具有奖励性质的刺激或事件，例如钱。进化使得初级强化物能够预测繁育成功，而学习使得次级强化物能够预测有望获得初级或其他次级强化物。

在这本书介绍技术的部分，我们一般会避免使用奖励、惩罚和强化物等术语，并用“奖励信号”、“强化信号”等代替，我们用这些术语来表示他们通常所指的含义。因为Rt是一个数字，而不是物体或事件，因此我们称之为“t时刻的奖励信号”，这种说法来自于神经科学，奖励信号被看作是脑内部的、影响决策的神经活动。

我们建议在15章当中将Rt看作是多个系统产生的、神经信号强度的总和，是抽象意义上的，用来测量状态和感知的奖励和惩罚特性。由于可以是正的、负的或者为零，因此最好将负的Rt叫做惩罚，而将为零的Rt叫做中性信号，但是为了简单起见我们一般会尽量避免使用这些术语。

在强化学习术语当中，产生Rt的过程定义了智能体试图解决的问题，这是一种评价信号，也就是说给智能体的行为打分，智能体的目标就是随着时间推移使得Rt的强度尽可能地大。如果我们将动物面临的问题看作是在一生当中尽可能多地获得初级强化物，那么Rt就像是该动物的初级强化物，从而使该动物可以通过进化解决真正的问题，也就是将基因传递给后代。预测未来将获得的累计Rt，或者说预测的变化，就类似于次级强化物。

奖励和惩罚可以激发动物的行为，使之增强或者减弱。但是奖励和惩罚也可能起不到强化作用，因为其作用不可能持续影响到未来的行为。例如奖励或惩罚可以让动物引起注意或者转移注意，引起趋近或回避行为，给予行动能量，或者激发某种情绪。奖励或惩罚只有对被施加者只的行为产生持续影响时才被视为强化物。而且并非所有的强化物都是奖励或惩罚。有时候，强化不是由于动物接受了某种刺激，并用好还是坏来评价行为的。行为模式可以受到刺激物的强化，不管该动物行为表现如何。强化物是否因动物之前的行为而获得，是工具性/操作性条件作用和经典/巴甫洛夫条件作用的主要区别。在两种条件下都可以使用强化这一术语，但是只有在前一种条件下，强化才可以看作是通过评价之前的行为而得到的反馈。

就像是奖励信号一样，强化信号可以是正的、负的或者为零。时刻t的强化信号是导致应对当前环境的策略或者价值函数改变的主要因素。具体来说，强化信号使参数成倍更新。对于一些算法来说，时刻t的强化信号等同于Rt（即时刻t的奖励信号），因为Rt是参数更新等式当中的关键乘数。但是我们这本书讨论的大多数算法，强化信号用于表述Rt以外的一系列术语。例如TD误差，是TD状态-价值学习当中的强化信号。为了将TD误差和心理学联系起来，Rt是初级强化物，而估计价值的时序差分，或者行动价值的时序差分，是次级强化物。因此只要γV (St)-V (St1) = 0，δt就代表纯粹的初级强化物，只要Rt=0，就代表纯粹的次级强化物。但是通常代表两种强化物的混合体。奖励信号和强化信号之间的差别，在第十五章我们讨论这些信号神经基础时是非常关键的。

著名的心理学家斯金纳及其追随者使用的术语可能带来混淆。斯金纳认为，当某个动物的行为后果增加了该行为发生的频率，那么就发生了正强化；当某个动物的行为导致令人厌恶的刺激得以消除，那么就发生了负强化；当行为导致了令人愉悦的刺激消失了，进而使得该行为频率降低了，那么就发生了负惩罚。我们发现没有必要这样区分，因为我们采用的术语更加抽象，奖励和强化信号可以取正值也可以取负值。（但是我们所说的强化信号取负值，不同于斯金纳所说的负强化。）

另一方面，我们仅仅用符号来区分数字代表的是奖励还是惩罚信号，而这无法反映动物的奖励/惩罚系统所具有的不同特点及脑机制。这提示了强化学习框架需要利用计算优势来区分奖励/惩罚系统，而这是我们目前尚未关注的。

另一个术语的差异是我们如何使用“行动”这个词。对于许多认知科学家来说，行动是有目的的，是基于动物对于行动及其后果之间关系的认识。一个行动（action）是目标指向的，是决策的结果，与此相反，反应（response）是由刺激诱发的。（为了统一）我们使用行动一词，而没有区分其他研究者采用的行动、决策及反应这些不同术语。这些术语存在重要差异，但是我们认为这些差异可以用模型无关和基于模型强化学习算法来概括，这一点我们将在14.7一节结合习惯性（habitual）和目标指向性（goal-directed）行为来讨论。

这本书还大量使用了控制（control）一词。我们所指的控制和大多数心理学家不同，指的是智能体影响其所处环境，从而带来其所希望获得的状态或事件：即智能体对其环境施加控制，因此和控制工程师的说法一致。心理学的控制通常则指动物的行为被接收到的刺激（刺激控制）或者体验到的强化所控制，是环境控制智能体。这种控制是行为矫正治疗的基石。当然，智能体在和环境交互作用中同时存在两种控制。和我们的观点相同，甚至更为人所知的是，智能体事实上控制着自身从环境所接受到的输入（Powers, 1973）。这和心理学家所谓的刺激控制不同。

有时候强化学习仅仅指的是从奖励（或惩罚）当中直接获得的学习策略，而不需要涉及价值函数或者环境模型。这就是心理学家所谓的刺激-反应（S-R）学习。不过对我们来说，和大多数当代心理学家一样，强化学习指代的范围远大于此，除了S-R学习，还包括价值函数、环境模型、规划和其他从属于心理功能的认知过程。

14.2预测和控制

本书描述的算法分为两类：用于预测的算法，和用于控制的算法。这是第三章介绍的强化学习问题解决方法当中自然产生的分类。这种分类在很多方面对应于心理学家的分类：经典（或巴甫洛夫）条件作用，以及工具性（或操作性）条件作用。考虑到心理学家对强化学习领域的影响，尽管这种对应关系不完全出于偶然，但是也颇令人惊讶，因为这些基于不同目的提出的概念之间彼此关联。这本书介绍的预测算法用于估计受智能体环境特征未来变化影响的变量。我们尤其关注的是，如何估计智能体在和环境交互作用过程中，预期未来可以获得的奖励。预测算法起到了策略评价算法的作用，属于用来改进策略的算法。但是预测算法的作用不限于预测未来奖励，它们也可以预测环境当中可以用数字赋值的特征（例如Modayil, White和Sutton, 2014）。预测算法和经典条件作用之间的对应关系在于，都可以预测未来的刺激，而这些刺激不一定是用于评价以往行动的奖励或惩罚。我们下一节将讨论经典条件作用。

工具性或操作性条件作用的情况有所不同。实验装置可以根据动物的行为，给予它们喜欢（如奖励）或者不喜欢（如惩罚）的刺激。动物会逐渐增加可以得到奖励的行为，减少会遭致惩罚的行为。用于强化的刺激是伴随（contingent on）动物的行为出现的，而经典条件作用当中的刺激则不是。工具性条件作用实验类似于第一章那些启发桑代克提出效果律的实验。控制是这类学习的核心概念，对应于强化学习策略改进算法所做的那样。

在这里，我们沿用心理学家的说法，指出经典条件作用和工具性条件作用的区别在于实验设置（实验装置是否根据动物的行为提供强化刺激），而不一定在于学习机制不同。实际上，很难将所有的反应依随性从实验中去除，并且这些实验涉及不同学习机制的程度也是个复杂的问题。从工程和人工智能的角度，预测算法和控制算法迥然不同，尽管很多强化学习方法同时包括两种算法。

在介绍工具性或操作性条件作用之前，我们将讨论经典条件作用，以及动物行为和时序差分估计之间的对应关系。

14.3经典条件作用

著名的苏联生理学家伊万·巴甫洛夫研究过反应是如何被无法生来就引发这种反应的刺激所诱发。

很明显在自然条件下正常的动物不仅会对可以带来即时好处或伤害的刺激进行反应，而且还会对其他物理或化学媒介——如声波、光波之类——进行反应，而这些媒介只是提示前面所提到的刺激临近了。这道理就像是，尽管并非看到或听到野兽本身会对小动物带来伤害，而是野兽的牙齿和爪子会伤害小动物，但是小动物还是会对前者做出反应（Pavlov, 1927, p. 14）。

巴甫洛夫（或者更准确地说，他作品的翻译者）将生来具有的反应叫做“无条件反应”（UR），将诱发这些反应的自然刺激叫做“无条件刺激”（unconditioned stimuli, US），而将预测刺激所激发的新的反应叫做“条件反应”（conditioned response, CR）。一个原本是中性的刺激，通常来说不会诱发强烈反应，当动物学习到这个刺激预示着无条件刺激的到来，就会引发无条件反应。这些术语现在仍被用于描述经典条件反应实验（尽管更好的翻译应该是“条件化”和“非条件化”反应（conditional/unconditional response））。

无条件反应经常具有保护性，就像应对眼睛刺激而产生的眨眼反应，或看到捕食者时的冻结反应。在多次体验到条件性刺激-无条件刺激之间的预测关系之后，动物就学会了条件刺激预测无条件刺激这一事实，从而会用条件反应来对条件刺激做出反应，保护自身免受无条件刺激的困扰，或者为无条件刺激的到来做准备。条件刺激有时候类似于无条件刺激，但是在时间上更早发生，且有效性更强。例如在研究甚广的一个饰演当中，作为条件刺激的音调总是会预测一股气流喷向兔子的眼睛（无条件刺激），从而诱发无条件反应——瞬膜（内眼睑）的保护性闭合。在多次重复之后，音调会在气流到来之前就诱发这个条件反应，并且在闭上内眼睑之前刚好气流喷向眼睛。这个无条件反应是因为预期气流到来而产生，并且比气流刚好喷到眼睛才闭眼起到了对眼睛更好的保护作用。通过学习刺激之间的预测关系，预期重要事件的到来，这种能力非常有帮助，并广泛存在于动物王国。

图14.1显示了两种经典条件作用下刺激的排列方式：在延迟条件作用当中，条件刺激持续发生在整个刺激间隔（interstimulus interval, ISI）当中，也就是条件刺激发生到无条件刺激发生之间的时间间隔。在痕迹条件作用当中，无条件刺激在条件刺激结束后开始，条件刺激结束到无条件刺激开始这段时间间隔叫做痕迹间隔。

在认识到条件反应如何预测无条件刺激的发生之后，科学家建立了一种基于时序差分学习的有影响力的模型。这种模型，叫做经典条件作用的TD模型，或简称TD模型，扩展了之前广为人知的和最具影响力的经典条件作用模型：Rescorla-Wagner模型（Rescorla andWagner, 1972）。在讨论TD模型之前，我们首先描述下Rescorla-Wagner模型，因为这些模型都遵循经典条件作用的基本理论。

14.3.1Rescorla-Wagner模型

[Rescorla](undefined)和Wagner用这个模型解释复合型条件刺激参与的经典条件作用，即包含了多个刺激成分（如同时出现的音调和闪光）的经典条件作用，在这种条件作用当中，每个刺激成分都可以用多种方式操纵。此类实验表明，假如一个动物已经学习到某个条件刺激预测某个无条件刺激（从而使得该动物针对该刺激产生一个条件反应），那么学习第二个附加的条件刺激就会更加困难。这叫做阻滞效应。该结果挑战了那种认为条件作用只依赖于时间依随性的看法，这种看法认为条件作用只需要满足一点，那就是无条件刺激通常在时间上紧跟条件刺激发生。与此相反，Rescorla-Wagner模型的核心观点是，动物只有当预期被打破的时候才会发生学习，换句话说，只有当动物感受到惊奇的时候（尽管这种惊奇不一定意味着动物有意识地抱有预期或者体验到惊讶的情绪）。

我们首先使用Rescorla-Wagner模型的术语和含义，然后再介绍我们描述TD模型时采用的术语及其含义。Rescorla和Wagner是这样描述他们的模型的。这个模型可以调整每个条件刺激的联结强度，这个强度代表这个条件刺激预测无条件刺激的明显程度或可靠性。[当条件刺激包括几个刺激成分时，每个刺激成分的联结强度随着复合刺激的联结强度而发生变化，而不仅仅代表每个刺激的联结强度本身](undefined)。

Rescorla和Wagner提出复合条件刺激AX，包括A和X成分，动物已经经历过A刺激，然后X刺激是新的。假如VA，VX，VAX分别代表A，X和AX的联结强度。设想一下某一次试验当中，复合条件刺激AX在无条件刺激Y之后发生。那么不同刺激的联结强度是这样变化的：

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image002.png)

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image004.png)

此处![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image006.png)和![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image006.png)是步长参数，大小取决于条件刺激和无条件刺激，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image009.png)是无条件刺激Y达到的渐近联结强度。（我们用![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image011.png)而不是Rescorla和Wagner所用的![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image013.png)，以免和我们之后采用的![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image013.png)混淆）。这个模型提出了更进一步的核心假设，即![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image015.png)这个联结强度等于![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image017.png)。

为了完善该模型，需要定义反应发生机制，也就是将V的值和条件反应对应起来。由于这种对应关系取决于具体的实验情境，Rescorla和Wagner没有具体指明对应关系，只是[假定更大的联结强度](undefined)V会产生更强的条件刺激，或者更容易产生条件刺激，而负的联结强度V意味着没有条件反应。

为了将Rescorla-Wagner模型扩展为TD模型，我们首先用自己的概念来重新描述该模型。具体来说，把我们使用的含义和线性函数逼近对应起来，并将条件作用过程看作是一种可以根据复合条件刺激来预测无条件刺激强度的学习（9.4节），也就是说我们将条件作用过程看作是一种学习过程，基于复合条件刺激，来预测无条件刺激的强度。无条件刺激的强度Y就是Rescorla-Wagner模型的![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image009.png)。我们再来介绍状态。Rescorla-Wagner模型是基于每次试验的模型，也就是描述每一次试验的联结强度，而不考虑这一次试验发生的其他事情，以及这一次试验到下一次试验中间发生的事情，因此实际上没有必要提到状态。如果纳入状态的话，我们可以将状态转换简化为TD模型，每一个状态就是根据每一次试验当中复合刺激的组合来标记这次试验。这对于[Rescorla-Wagner](undefined)模型来说过于复杂，但是对于我们来说可以接受因为当我们将Rescorla-Wagner模型扩展为TD模型之后这种做法还会带来好处。

因此，假定试验类型，或者说状态*s*，可以用表示特征的真值向量来描述，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image019.png)，其中如果条件刺激CSi存在，那么![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image021.png)=1，否则![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image021.png)为0。如果联结强度的*n*维向量是![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image024.png)，那么对于试验类型*s*，合并联结强度就是

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image026.png)                                                                                                                        (14.1)

这对应于强化学习的估计价值，我们也可将其看作无条件刺激的预测值。

现在我们先让*t*代表一次完整试验的序号，而不是*t*通常所指的时间步骤（当我们将*t*的通常含义扩展到TD模型的时候我们会再次使用*t*的通常含义），并假定St是对应于试验*t*的状态。如果试验*t*当中，联结强度向量![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image028.png)被更新为![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image030.png)，如下式：

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image032.png)                                                                                                                 (14.2)

此处![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image034.png)是步长参数。此外由于我们描述的是Rescorla-Wagner模型，所以![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image036.png)就是估计误差

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image038.png)                                                                                                                     (14.3)

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image040.png)是试验*t*的预测目标，也就是无条件刺激的强度，或者用Rescorla-Wagner模型的说法，是无条件刺激在所在试验当中的联结强度。应当注意，由于(14.2)当中存在![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image042.png)，所以只有条件刺激成分的联结强度会根据该试验的结果而调整。你可以将估计误差看作是测量惊讶程度，当不符合目标无条件刺激强度的时候，动物对合并联结强度的预期就被打破了。

Rescorla-Wagner模型是一种误差校正的学习规则，很像是机器学习当中其他的监督学习规则。事实上，它和最小均方法（LMS），或者Widrow-Hoﬀ学习规则（Widrow和Hoff，1960）本质上相同，都是要找到一个权重（也就是这里的联结强度），使得所有误差平方的均值尽可能地接近0。这是一种曲线拟合方法，或回归方法，这种算法在工程学和科学应用当中使用广泛。（LMS和Rescorla-Wagner模型的唯一区别在于，对LMS来说，输入向量![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image044.png)可以对条件刺激成分取任何真值，并且——至少对于LMS规则的最简单形式来说——步长参数![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image034.png)不依赖于输入向量或者作为预测目标的刺激本身。）

误差校正可以解释许多使用复合条件刺激的经典条件作用。比如说，在阻滞实验当中，如果新的成分加入到已经被条件化了的复合条件刺激当中，那么对于新的复合刺激进行条件化，对于新增加那部分条件刺激的联结强度，几乎起不到增进作用。以前的学习会阻碍对新增部分的学习，因为误差会总是削减至零或者较低的值。因为已经预测到会发生无条件刺激，因此增加新的条件刺激不会产生惊奇感。

尽管Rescorla-Wagner模型对阻滞效应，以及其他经典条件作用实验当中动物行为提供了简单和有力的解释，但是远非一个完美的或者完整的模型。还有其他观点可以解释一系列其他的效应，对经典条件作用幽微之处的理解还在不断加深。其中一个方向涉及到刺激的时间设定。Rescorla-Wagner模型当中的步骤*t*代表一次条件作用试验。但是对于一次试验当中发生了什么，这个模型无法解释细节。在每次试验当中，动物可能会体验到许多持续时间不同的刺激，不同刺激的时间关系极大地影响着动物的学习。

14.3.2TD模型

经典条件作用的TD模型是Rescorla-Wagner模型的扩展形式。不仅可以解释所有该模型能够解释的行为，而且还可以解释发生在一次试验当中和不同试验之间刺激的时间关系。经典条件作用的TD模型是实时模型，而Rescorla-Wagner模型则是单次试验模型。

为了描述TD模型，我们一开始先描述下Rescorla-Wagner模型，但是这里所说的状态，不仅仅是一次试验当中的条件刺激成分。我们将试验看作是一次试验或者多次试验当中的一系列状态，发生于一个小的时间单位比如说0.01秒。实际上我们可以完全放弃试验的概念。从动物的角度来说，一次试验就是和世界持续交互作用过程中的一个片段而已。沿用我们通常看待智能体和环境交互作用的角度，想象下动物正在持续不断地经历一系列状态*s*，每个状态用一个特征向量![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image047.png)代表。在这种情况下，我们模拟理想化的经典条件作用，动物完全不可能控制这一系列状态。这样表述状态/特征向量的结果之一就是特征不但可以用来描述动物经历的外部刺激，而且还可以描述外部刺激在动物脑中产生的神经活动模式。当然了，我们不可能确切知道这些神经活动模式是什么，但是类似TD这样的模型可以让我们探究不同的对外部刺激进行内部表征的假设，以及在学习方面从这些假设出发得到的推论。

接下来我们将描述一些TD模型当中使用过的状态表征及其意义，不过目前来说我们对于表征还持不可知论，仅仅假设每种状态*s*由一种特征向量![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image019.png)来表征。由此状态对应的合并联结强度由式子(14.1)给出，和Rescorla-Wagner模型类似，不过TD模型对于联结强度![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image024.png)的更新方式有所不同。如果*t*代表时间步骤，一个短的时间间隔，而不是完整的一次试验，那么TD模型是这样对学习进行更新的：

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image050.png)                                                                                                                        (14.4)

在这里![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image052.png)代替了Rescorla-Wagner更新的![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image054.png)，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image052.png)是一个资格迹向量，而且和(14.3)的![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image056.png)不同，这里的![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image056.png)代表TD误差：

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image058.png)                                                                                           (14.5)

此处![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image060.png)代表折扣因子（介于0到1），![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image062.png)代表*t*时刻的估计目标，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image064.png)和![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image066.png)是*t*+1时刻的合并联结强度，其表达式见(14.1)，但是这里*t*代表时间步骤而不是试验序号。资格迹et的每个成分*i*随着特征向量![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image068.png)的成分![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image070.png)而增加或减少，否则就会以一定的速率衰退，其衰退速率取决于![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image072.png)：

![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image074.png)                                                                                                                     (14.6)

这里![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image013.png)代表通常的资格迹衰减参数。

注意如果![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image060.png)=0，那么TD模型实际上就简化为Rescorla-Wagner模型，仅有一点不同的是：*t*的含义在两种情形下不同，Rescorla-Wagner模型中代表试验序号，而TD模型中代表时间步骤，并且在TD模型当中预测目标![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image078.png)要领先一步。TD模型等价于用线性方程逼近的方法对半梯度的TD(![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image013.png))算法进行回溯（见12章），此外不同于TD算法通过学习价值函数改进策略，这里![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image040.png)不一定等于奖励信号Rt。

14.3.3TD模型的模拟

像TD模型这样的实时条件作用模型很有意思，因为它们可以对一些单次试验模型无法表达的情形进行预测。这些情形可以根据条件刺激的发生和持续时间，与条件反应的发生时间和形态而呈现不同特点。复合条件刺激的成分可以一起发生但不一定严格同步。并非同时开始和结束的复合条件刺激成为序列条件刺激。几乎所有的学习都包含着序列条件刺激，这可能由于动物会将试验者眼中的单一刺激区分为早期和晚期成分，也可能由于动物的行为可以导致一系列可以预见的情形，从而获得强化。

序列复合条件作用TD模型的预测取决于一些因素：参数![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image034.png)、![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image013.png)、![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image060.png)的值，反应发生机制的选择，以及最重要的，试验当中刺激呈现方式可以表征为状态特征向量。图14.2展示了三种根据TD模型考察动物行为的刺激呈现方式：完全序列复合刺激（complete serial compound, CSC），微刺激（microstimulus,MS），以及存在（presence）表征(Ludvig, Sutton,and Kehoe, 2012)。这些表征方式在泛化到相邻的刺激出现的时间点时程度有所不同。

图14.2当中最简单的表征方式就是存在表征。这种表征方式对于复合条件刺激的每种成分都有一个特征与其对应，当成分存在那么特征等于1，当成分不存在特征等于0。（按照我们的表述，每次试验每个时间点*t*具有不同的状态St，对于包含了*n*个成分的复合条件刺激，如果这些成分在不同时间发生，持续时间不同，那么当i=1,...,n的时候，每种成分CSi都具有特征![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image085.png)，当CSi存在的时候![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image087.png)，否则为0。）存在表征并不是动物脑中刺激如何表征的现实假设，但是就如我们接下来将要描述的，采用存在表征的TD模型，可以构建出经典条件作用当中许多和时间有关的现象。

对于CSC表征（图14.2左栏），每个外部刺激的发生都会激发一系列发生时间明确、持续时间短的内部信号，直到外部刺激结束。（按照我们的表述，对于一次试验当中每个复合刺激成分CSi，对这次试验的每个时间点*t*来说，都有独立的特征![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image089.png)，当存在CSi的任何一个时刻t’的时候，如果t=t'，那么![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image091.png)，否则=0[[1\]](#_ftn1)。）这就像是假定动物的神经系统有一个时钟，在刺激呈现的时候严格记录时间，即工程师所谓的“抽头延迟线”。和存在表征一样，CSC表征也不是现实的模型，而是对脑的内部如何表征刺激提出的假说，不过Ludvig等（2012）称之为“有用的传说”，因为当相对不受刺激表征限制的时候，它可以揭示很多关于TD模型如何工作的细节。CSC表征也在大多数多巴胺神经元的TD模型当中得到应用，我们将在15章进行讨论。CSC表征经常被看作TD模型的必要成分，尽管许多人对这种观点存在误解。

MS表征（图14.2中间一栏）就像是CSC表征，每个外部刺激都会引发一系列内部刺激，但是内部刺激——微刺激——在形式上并非数量有限、也并非互不交叉，这些微刺激是在时间上延续并且互相重叠发生的。当刺激发生后随着时间的推移，不同的微刺激会变得更加活跃或者不活跃，随后的每种微刺激会逐渐在时间上持续更久，最大强度逐渐减弱。当然，根据微刺激的性质不同有很多类型的MS表征，一些MS表征在文献中有所涉及，一些文献提到了动物脑可能产生微刺激（见章末参考文献和历史评述）。MS表征比存在表征或CSC表征更接近对刺激的实际的神经表征，还可以将TD模型的行为和动物实验的更多现象联系起来。特别是，假定无条件刺激和条件刺激一样发动了一些列微刺激，而且还研究微刺激、资格迹、折扣之间的交互作用对于学习的显著影响，因此可以通过TD模型构建假说来解释经典条件作用的许多微妙现象，以及解释动物脑是如何产生这些现象的。我们后面将会有更详细的介绍，特别是第15章探讨强化学习和神经科学的时候。

即便对于简单的存在表征，TD模型也可以产生Rescorla-Wagner模型能够解释的经典条件作用的全部基本特性，此外还可以产生单次试验模型范畴以外的条件作用特征。比如，经典条件作用的一个显著特征是无条件刺激通常在中性刺激发生后开始，在条件化之后，条件反应在无条件刺激开始之前就会开始。换句话说，条件化需要一个正的刺激间隔时间，条件反应通常预示着无条件刺激的到来。条件作用的强度（比如条件刺激引发条件反应的比例）与刺激间隔之间的关系在不同物种和反应系统之间差异很大，但是通常在刺激间隔为零或为负数的情况下可以忽略（比如当无条件刺激在条件刺激同时或者早些时候发生），当刺激间隔为正数的时候条件作用的效果最好，这时候刺激间隔对条件作用的影响也最大，当一段时间间隔之后降为零。这段时间间隔在不同反应系统之间差异较大（尽管研究发现联结强度有时候增长较小，甚至当刺激间隔为负的时候也变为负数）。TD模型当中这两者之间确切的关系取决于刺激呈现的参数和其他细节，但是无论如何，由刺激间隔所决定都是TD模型的核心特征。

从序列复合条件作用当中衍生出的理论问题还涉及远距离联想的易化效应。有研究发现，当条件刺激和无条件刺激之间那段时间间隔出现第二个条件刺激，从而构成序列复合刺激的时候，第一个条件刺激的条件作用就会得到易化。图14.3显示了存在表征的TD模型行为，该实验的时间细节显示在图上方。和实验结果一致（Kehoe, 1982），模型显示了条件化速率提高，以及当出现第二个条件刺激的情况下第一个条件刺激的渐近条件化程度提高。

关于一次条件化试验当中刺激之间时间关系的的效应，Egger和Miller有一个著名的实验。在这个实验当中，有两个时间上相互重叠的条件刺激，如图14.4。尽管条件刺激B和无条件刺激的时间关系更为紧密，但是相对于没有条件刺激A的情况，条件刺激A的存在使得条件刺激B条件化程度更低。图14.4下图反映了TD模型对该实验的模拟，结果也是一致的。

TD模型的另一个和刺激时间有关的特征也值得关注，因为模型可以正确地预测经典条件作用的某些方面，而TD模型发现之初没有发现此规律。TD模型（存在表征和其他更为复杂的表征）预测说，如果被阻滞的刺激在时间发生上提前到阻滞刺激发生之前，那么阻滞效应就会发生逆转。我们回忆下，在阻滞效应当中，动物已经学习到一个条件刺激预测一个无条件刺激，还学习到增加的一个条件刺激对无条件刺激的预测作用将会减弱，也就是说被阻滞了。但是如果这个增加的条件刺激比之前的条件刺激发生更早，那么根据TD模型，新的条件刺激学习作用未被阻滞；事实上，当继续加以训练，增加的条件刺激联结强度会增加，而之前的那个条件刺激联结强度会减弱。TD模型的这种行为见图14.5。这个模拟实验和Egger-Miller实验不同点在于，发生较晚的持续时间较短的条件刺激先进行条件化，直到和无条件刺激建立连接。这个预期结果使得Kehoe、Scheurs和Graham(1987)用兔子进行了瞬膜反射实验。他们的结果证实了模型的预期，他们还注意到非TD模型则很难解释他们的实验结果。

TD模型预测说较早发生的预测刺激优先于较晚发生的预测刺激，那是因为就像这本书介绍的所有预测方法一样，TD模型是基于一种备份（back-up）思想：对联结强度的更新使得某一状态下的强度转向该状态下的备份强度。备份的另一个结果是，TD模型可以解释次级条件作用，这是经典条件作用的一种特征，是Rescoral-Wagner 模型和类似模型无法解释的。次级条件作用是这样的：一种先前条件化的条件刺激可以作为无条件刺激，使得另一个中性刺激条件化。巴甫洛夫描述了一个实验，他的助手训练一条狗对于可以预测食物（无条件刺激）的节拍器流口水。然后在狗的视线范围内放置一个黑色方块，方块出现后紧接着传出节拍器的声音，但是没有出现食物。10次试验之后，狗在看到黑色方块的时候就开始流口水，不管看到方块之后是否有食物。节拍器的声音本身作为了流涎反应的强化物。次级条件作用也发生在工具性条件作用当中，这时候总是预测奖励（或惩罚）的刺激变成了奖励（或惩罚）本身，从而成为次级强化物（或条件化的强化物）。

图14.6显示了次级条件作用实验当中TD模型的行为（存在表征）。在第一阶段（图中未显示），条件刺激B和无条件刺激进行结合。第二个阶段，条件刺激A和条件刺激B在没有出现无条件刺激的情况下结合，就像是图14.6当中出现的那样。哪怕没有和无条件刺激结合，条件刺激A也具有了联结强度。在TD模型当中，条件刺激A首先获得了充分的联结强度，然后该强度和条件刺激B的联结强度都减弱了。动物实验也发现了类似的行为模式。

TD模型可以产生类似于次级条件作用的行为，由于![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image093.png)在TD误差![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image036.png)（公式14.5）当中也出现了。这意味着由于之前的学习，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image096.png)和![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image098.png)不同（时序差分），使得![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image036.png)不为零。这种差异和![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image100.png)（公式14.5）的作用类似，都意味着只要涉及到学习，时序差分就等同于无条件刺激的出现。事实上TD算法的这个特征正是提出它的主要理由，目前为止我们是通过和第六章描述的动态规划联系起来理解TD算法的。而备份价值和次级条件作用（以及高级条件作用）密切相关。

在TD模型描述的行为实例当中，我们只是考察了条件刺激成分联结强度的变化；我们没有考察该模型可以预测动物条件反应的哪些特性，比如说时间、形态以及在不同试验当中如何形成。这些特性和物种、所观察的反应系统以及条件化试验参数有关，但是在很多采用不同动物和反应系统的实验当中，条件反应的强度，或者发生概率，随着条件刺激发生时间的临近是会增长的。例如，在兔子瞬膜反射的实验当中，随着试验次数增加，距条件刺激开始到瞬膜开始遮盖眼球之间的延迟时间会降低，在条件刺激和无条件刺激之间的间歇，瞬膜闭合程度逐渐增加，直到在预期无条件刺激发生的时间点达到最大闭合程度。条件反应的时间和形态极具有适应价值，太早闭眼会降低视力（尽管瞬膜是透明的），而太晚闭眼则会失去对眼球的保护价值。理解条件反应的这些特征对于构建经典条件作用的模型是具有挑战性的。

TD模型没有在定义当中，将无条件刺激预测的时间进程![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image098.png)转换为可以和条件反应特征相比较的维度。最简单的做法是让模拟的条件反应时间进程和预测无条件刺激的时间进程相同。这种情况下模拟的条件反应，及其如何在不同试验当中变化只取决于刺激表征方式和模型的参数值![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image034.png)、![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image060.png)以及![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image013.png)。

图14.7反映了在图14.2提到的三种刺激表征方式下，在学习的不同时间点预测无条件刺激的时间进程。根据Ludvig等（2012），无条件刺激在条件刺激发生之后25步发生，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image106.png)，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image108.png)，![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image110.png)。对于CSC表征（图14.7左图）来说，通过TD模型学习到的无条件刺激预测曲线在条件刺激和无条件刺激时间间隔之内呈现指数级增长，直到在无条件刺激出现时达到最大值（在第25步）。这种指数级增长是TD模型学习规则折扣的结果。对于存在表征（图14.7中图）来说，当刺激出现的时候，无条件刺激预测基本不变，因为每种刺激只有一个权重，或者说联结强度。因此，存在表征的TD模型不能重现条件反应的很多时间特征。对于MS表征（图14.7右图）来说，TD模型建立无条件刺激预测更为复杂，200次试验之后无条件刺激预测曲线达到合理的近似程度。

图14.7呈现的无条件刺激预测曲线并非准确对应于特定某类动物实验的条件反应建立过程，但是也展示了刺激表征方式对于TD模型预测的显著影响。此外我们只提一点，刺激表征方式和折扣相互作用，对于决定基于TD模型的无条件刺激预测特征非常重要。另一个我们不能展开讨论的方面是无条件刺激预测条件反应特征的不同的反应产生机制，图14.7当中呈现的是原始的无条件刺激预测模式。即便没有假设动物脑可以从无条件刺激预测当中产生外涎反应，但是图14.7显示的在CSC和MS表征下的模式，会随着无条件刺激的临近而增加，直到无条件刺激发生时达到最大值，这一点正如在许多动物条件作用实验当中看到的那样。

当把TD模型和特定的刺激表征方式结合起来，反应产生机制就可以解释动物经典条件作用实验当中的大量现象，其解释力之广泛令人吃惊，但是TD模型仍然远非一个完美的模型。为了生成许多经典条件作用的细节，需要扩展模型，加入基于模型的成分和机制，来相应地改变某些参数。另外还可以采取和Rescorla-Wagner的误差校正方法迥然不同的其他方法，来模拟经典条件作用。比如说概率论框架下的贝叶斯模型，经验可以修正概率估计。所有这些模型都可以有效增进我们对经典条件作用的理解。

也许TD模型的最显著的特征是基于理论，就是我们在本书所提及的，动物的神经系统在条件作用当中试图在做的，是在刺激表征方式和神经系统运作方式的约束之下，准确地预测长期结果。换句话说，TD模型是一个经典条件作用的规范化模型（normative account），该模型的核心特征是进行长期而不是短期预测。

以模拟动物学习行为细节为外显目标，经典条件作用的TD模型是其中一例。除了作为算法，TD学习还是生物学习的基本模型。正如第十五章所讨论的，TD模型已经作为多巴胺神经元活动的有影响力的模型，多巴胺是哺乳动物脑分泌的、参与奖励加工的化学物质。这些都是强化学习理论和动物行为、神经科学数据紧密结合的实例。

现在我们将开始考虑强化学习和动物学习在工具条件作用实验当中的对应关系，工具条件作用是心理学家研究动物学习的另一类重要实验。

14.4工具性条件作用

在工具性条件作用实验当中，学习有赖于行为结果：强化刺激是根据动物的行为而施加的。与此相反，在经典条件作用当中，强化刺激，也就是无条件刺激，是不依赖于动物行为而施加的。工具性条件作用通常被认为和操作条件作用相同，而后者是斯金纳（1938，1961）针对行为依随性强化实验提出的，尽管两个术语的实验和理论在一些方面有所不同。在行为依随性强化实验当中，我们统一用工具性条件作用这个词。工具性条件作用可以追溯到100多年前美国心理学家爱德华·桑代克的实验。在这些实验当中，桑代克观察把猫放到迷笼后猫是如何通过恰当行动逃脱的（图14.8）。例如，猫可以连续完成三个独立的动作，来打开迷笼的门：按下门背后的台板，用爪子拉线，然后上下推动一根板条。当第一次被放进迷笼时，可以看到食物在外面，只有少数猫表现出明显的不适感，剧烈活动来试图逃脱这种束缚（Thorndike, 1998）。

在不同的实验当中，逃脱的方法可能不同。桑代克记录了每只猫逃脱箱子使用的时间。他发现对于不同的猫，随着经验的累积逃脱时间几乎都减少了，从300秒到六七秒。他是这样描述猫在迷笼当中行为的：

猫极为冲动地用爪子到处抓笼子试图逃脱，最终会抓到直线、线圈或者按钮从而打开门。渐渐地，猫会停止尝试其他不成功的做法，只保留下成功的行动，并带来愉悦感，直到许多次之后，将猫放置在笼子里面之后，猫必定会立刻抓挠按钮或者线圈(Thorndike 1898, p. 13)。

这样一些实验（有时还包括狗、鸡、猴子甚至鱼的实验）使得桑代克最终形成了一些学习定律，其中影响最大的是效果律，就是我们在第一章引用的。这条定律描述了试错学习法。正如第一章提到的，效果律的很多方面都引起了争议，定律的细节也在之后若干年几经修改。尽管如此，不同形式的效果律仍然成为了一直以来的学习法则。

强化学习算法的本质特征恰好对应着动物学习的效果律。首先，强化学习算法是具有选择性的，也就是说，学习是通过比较不同的结果来选择行动的。第二，强化学习算法是具有联结性的，这意味着发现的行动选择和特定情境或状态有关，这就构成了智能体的策略。类似于效果律，强化学习不仅是发现获得大量奖励的行动过程，而且还将行动和情境或状态联系起来。桑代克使用的学习一词，代表着“选择和联结”(Hilgard, 1956)。进化论的自然选择可以看作选择的基本实例，但是不具有联结性（至少由于该选择广为人知）；监督学习具有联结性，但是不具有选择性，因为监督学习需要指导来告诉智能体如何改变行为。

用计算科学的术语，效果律描述了将搜索和记忆结合的初级方法：通过尝试和选择每种情境下的不同行动来进行搜索，而记忆则将情境和行动联系起来，以此来做出这些情境下的最优行动。不管记忆是以策略、价值函数还是环境模型的形式存在，搜索和记忆都是强化学习算法的本质成分。

强化学习算法需要搜索，意味着需要以某种方式进行探索（explore）。动物显然也需要探索，早期的动物学习研究者对于动物在类似桑代克的猫迷笼条件下选择行动所需的指导意见不一。行动是“绝对随机的、盲人摸象”的结果(Woodworth, 1938, p. 777)，还是需要一定程度的指导，不管这种指导来自于先前的学习、推理或是其他途径？尽管一些人包括桑代克持有第一种立场，另一些研究者更倾向于后一种观点，即刻意的（deliberate）探索。事实上，一些问题解决实验当中，动物表现出一种洞察力（insight），因为它们解决问题的速度相当之快，有时候不需要身体的探索，它们似乎就思考出了（figure out）答案。强化学习算法在智能体选择行动时借助多少指导，规定较为宽泛。我们在本书中用过的探索方法比如![img](file:///C:\Users\lemon\AppData\Local\Temp\msohtmlclip1\01\clip_image112.png)-贪婪搜索，以及上置信区间行动选择（upper-confidence-bound action selection）就是最简单的方法。也有更为复杂的方法，仅需限定算法具有某种形式的探索。基于模型强化学习方法是通过模拟过去的经验进行学习的，例如Dyna结构（8.2节），就是一种表现出洞察力的外部观察者，但是仍然需要探索来学习准确的模型。

根据环境的当前状态选择行动的强化学习方法，其特征和桑代克在猫迷笼中观察到的猫的行为类似。猫从它们当前情境下自发的行动当中加以选择，桑代克称之为“本能冲动”。第一次被放入笼中时，猫出于本能用力地挠、抓和咬，这是猫在活动受限的空间当中本能的反应。从这一系列行动当中猫选择了得以成功逃脱的那部分行动，而不是所有可能的行动。这就像是我们所说的，从状态s当中选择行动a，a属于一系列合理的行动集合A(s)。指定这一行动集合是强化学习的重要方面，因为可以极大地简化学习过程。这一集合就像动物的本能冲动一样。另一方面，桑代克的猫还可能根据情境对行动加以排序，从中进行探索，而不是从所有本能冲动当中加以选择。这又简化了强化学习过程。

受到效果律影响的、最声名卓著的动物学习研究者包括了克拉克·霍尔（Hull, 1943）和B.F.斯金纳（Skinner,1938）。其核心研究就是基于行动结果选择行为的思想。强化学习和霍尔的理论有诸多相似，比如在解释行动和之后的强化刺激存在较长时间间隔的情况下为何仍然具有学习能力，提出了资格和次级强化的概念（14.5节）。随机性在霍尔的理论当中也占有一席之地，他称之为“行为振动”用来介绍探索行为。

斯金纳不完全同意效果律的记忆这部分内容。他很反感联结的思想，并强调自发发动的行为。他提出了术语“操作性”来强调行动对于环境的作用。桑代克及其他人的实验包含了一系列可分离的试验，与此不同，斯金纳的操作条件作用实验允许动物的行为在很长时间内不被打断。他发明了操作条件作用实验箱，现在被称为“斯金纳箱”，最简单的形式包括了一个杠杆或按键，根据精心制定的规则——强化日程，动物可以按压并获得奖励（如食物或水）。通过记录按压杠杆的累积次数随时间变化的规律，斯金纳及其追随者考察了不同的强化日程对于动物按压杠杆频率的影响。目前尚没有很好的利用计算领域的强化学习原则来模拟这些实验结果，不过也有例外，具体可见章末参考文献及历史评述。

斯金纳的另一个贡献在于，他认识到通过对理想的行为进行渐进强化可有效训练动物，他称为塑造（shaping）。尽管这项技术也被其他人用过，包括斯金纳本人，他和他的同事在训练鸽子用喙来滚木球时仍然对这项技术的重要性印象深刻。在等待了很长时间没有看到他们想要强化的击球动作之后，他们

*……**决定强化任何和击球动作类似的动作——**一开始也许仅仅是看那颗球，然后选择更接近目标动作的反应加以强化。结果令人惊讶。几分钟之后，鸽子不断让球击中盒子的侧壁又反弹回来，就好像一名冠军选手那样(Skinner, 1958, p. 94)**。*

鸽子不仅可以学会不寻常的行为，而且对于行为及其强化依随性彼此变化的过程学习得很快。斯金纳将强化依随性的变化过程比作雕塑家将粘土塑造为理想的形象。塑造是计算强化学习的一种强大的技术。当智能体很难接收到任何非零的奖励信号时，无论这种情况是由于奖励很稀有，还是由于最初的行为不能获得奖励，都可以在一开始先解决较简单的问题，然后慢慢增加问题的难度，这种方法是一种有效的、有时候甚至是必不可少的策略。

另一个和工具性条件作用密切相关的心理学概念是动机，指的是影响行为方向和强度（或称活力）的过程。桑代克的猫动机是逃脱迷笼，因为它们想要获得笼子外面的食物。因此达到这个目标对猫来说是一种奖励，可以强化逃脱行为。很难将多维度的动机概念，准确地和计算角度的强化学习联系起来，但是某些动机维度确实和强化学习有联系。

在某种意义上，强化学习智能体的奖励信号构成了动机的基础：智能体有动机去使得长期来看的奖励总和最大化。动机的一个核心方面，就是让智能体体验到奖励。在强化学习当中，奖励信号取决于智能体的环境状态及其行动。另外，正如第一章提到的，智能体的环境状态不仅包括了机器外部的信号，比如装置着智能体的有机体或机器人，而且还包括内部环境状态。一些内部状态正好对应于心理学家所说的动物的动机状态，影响着哪些因素对动物而言构成奖励。比如，一只动物饥饿的时候，进食更有可能成为奖励，而当这只动物刚好进食了满意的一餐，则进食不太可能成为奖励。状态依存性的概念，可以涵盖很多影响奖励信号产生的调节因素。

价值函数可以进一步和心理学的动机概念联系起来。如果行动的最基本动机是获得尽可能多的奖励，那么对于一个强化学习智能体而言，利用价值函数选择行动，最为类似的动机就是沿着价值函数上升梯度。选择那些下一个状态价值最高的行动（或选择具有最大行动价值的行动，这两者本质上相同）。对于智能体来说，价值函数是决定行为方向的最主要的驱动力。

另一个动机维度是动物的动机状态不仅会影响学习，而且会影响动物行为的强度或者说活力。例如，当学会在迷宫的目标盒子当中寻找食物后，饥饿的老鼠会比不饥饿的老鼠更快地跑向目标盒子。这和强化学习框架的联系不是很明显，但是章末的参考文献和历史评述我们引用了几篇文献，这些文献基于强化学习提出了行为活力理论。

我们现在再来讨论下，当强化刺激发生在要强化的事件之后，学习如何发生。强化学习算法的机制，比如资格迹和TD学习，可以解释延迟强化条件下的学习现象。而延迟强化和心理学家对相应条件下动物学习的假设是一致的。

14.5延迟强化

效果律可以对联结产生一种回溯的效果，早期的批评者无法理解当前怎么会影响过去。学习可以在行动和随后奖励/惩罚之间产生显著延迟这种现象，更加剧了这种担忧。简单来说，在经典条件作用当中，当无条件刺激紧随条件刺激结束，学习就发生了。我们将这类问题叫做延迟强化，和Minsky（1961）提出的“学习系统的信用分配问题”有关。强化学习算法可以通过两种基本机制解决这个问题。第一种叫做资格迹，第二种是利用TD方法学习价值函数，用于即时评价行动（在工具性条件作用当中），或预期目标（在经典条件作用中）。两种方法对应于动物学习理论的类似机制。

巴甫洛夫（1927）指出，每种刺激都会在神经系统当中留下痕迹，这种痕迹在刺激结束后仍会留存一段时间，当条件刺激消失和无条件刺激开始时间存在一段间隔，那么痕迹就会使个体产生学习。现在把这种条件作用称为痕迹条件作用（图14.1）。假定条件刺激的痕迹在无条件刺激出现后仍然存在，那么学习就会在痕迹和无条件刺激同时存在的情况下发生。我们将会在第十五章讨论神经系统如何产生痕迹的机制。

刺激痕迹还被认为是一种弥合行动和随后奖励/惩罚之间时间间隔的方式。在Hull颇具影响力的学习理论当中，用“克分子刺激痕迹（molar stimulus traces）”来解释他提出的动物目标梯度，目标梯度就是随着强化的逐渐消退，工具性反应的最大强度也随着降低(Hull, 1932, 1943)。Hull假定动物的行动会留下内部刺激，这种刺激痕迹在行动发生后，随着时间推移呈现指数级消退。通过观察动物学习的数据，他假定痕迹会在30到40秒之后降为零。

本书算法当中使用的资格迹类似于Hull的痕迹，也就是过去状态访问后留下的不断消褪的痕迹，或者说过去的状态-行动对的痕迹。资格迹是由Klopf（1972）在神经元理论当中提出的，Klopf认为突触当中保留着神经元之前活动的痕迹，在神经元之间建立起联系。Klopf的痕迹说比我们的算法采用的指数级衰减痕迹更加复杂，我们将会在15.9节当中更加详细地讨论他的理论。

为了解释较长时间的、跨越刺激痕迹的目标梯度，Hull（1943）指出较长的梯度产生于从目标反向传递的次级强化，该过程和克分子刺激痕迹同步进行。动物实验表明如果在消退期内实验条件有利于发展次级强化，那么学习不会像在次级强化被阻断的条件下那样消退得那么快。如果在消退间隔内刺激有规律地出现，就有助于发展次级强化。这时候因为次级强化更加及时，就好像奖励没有被推迟一样。为此霍尔设想基于初级强化的延迟作用，存在一个初级梯度，这个梯度受到刺激痕迹的影响，并由于次级强化作用而改变和延长。

这本书介绍的算法针对延迟强化使用了资格迹和价值函数来支持学习，和Hull在此种情况下对于动物学习的假设一致。13.5，15.7和15.8介绍的行动者-评价者模型清楚地说明了这种对应关系。评价者采用TD算法学习和系统当前行为有关的价值函数，从而预测当前策略的结果。行动者基于评价者的预测来更新策略，或者更准确地说，基于评价者预测的变化。评价者得到的TD误差作为行动者的次级奖励信号，提供了一种对行动效果的即时评价，哪怕初级奖励信号大大延迟。估计行动价值函数的算法，比如Q学习和Sarsa，仅仅使用TD学习原则，借助次级强化来实现延迟强化的学习。TD学习和多巴胺神经元活动之间的紧密对应关系我们将在十五章当中加以讨论，这部分内容将进一步支持强化学习算法和Hull学习理论之间的联系。

14.6认知地图

基于模型的强化学习算法可以利用和心理学家所谓认知地图类似的环境模型。从第八章关于规划和学习的讨论中可以知道，环境模型指的是智能体用于*预测环境将如何通过状态变化和奖励给予行动反馈*的任何信息。规划指的是用于从环境模型当中计算策略的过程。环境模型包括两个方面：状态转换模型，用于编码行动是如何导致状态变化的，奖励模型，用于编码一类知识，即针对每种状态或每个状态-行动对，预期可以得到什么样的奖励信号。基于模型算法，是采用预测行动可能结果的模型（包括将来状态和奖励信号）来选择行动的。最简单的规划，是比较想象当中一系列决策的预期结果。

动物是否利用环境模型，如果是的话，如何利用这些模型在动物学习研究史当中具有重要影响。一些研究者质疑动物学习和行为的刺激-反应观（S-R），这种观点和最简单的模型无关学习策略类似，他们的证据是潜伏学习现象。在最早的潜伏学习实验当中，让两组老鼠跑迷宫。对于实验组来说，第一阶段没有奖励，但是第二阶段食物会突然出现在目标箱子。对于控制组来说，两个阶段目标箱子始终都有食物。问题在于实验组老鼠是否在第一阶段没有食物奖励的时候学会了什么。尽管第一阶段这些实验组老鼠看起来不像是学到了很多东西，但是第二阶段只要他们发现可以获得食物，那么就会快速赶上控制组老鼠的成绩。因此研究者得到结论说“在没有奖励的阶段，[实验组]老鼠对于迷宫产生了潜伏学习，因此可以在受到奖励之后快速利用这些知识” (Blodgett, 1929)。

潜伏学习和心理学家爱德华·托尔曼的研究关系最为密切，托尔曼对这类结果的解释是，哪怕没有奖励或惩罚，动物也可以学会“关于环境的认知地图”，并且可以在有动机驱动的情况下利用认知地图达到目的(Tolman,1948)。认知地图可以使老鼠计划达到目标的路线，而这条路线和他们之前探索的不同。这样的解释长期受到争议，争议的核心就是心理学当中的行为主义和认知主义分野。用现在的话来说，认知地图不仅仅限于空间位置模型，而且是更为一般的环境模型，或者说动物的“任务空间”模型(比如Wilson, Takahashi, Schoenbaum和Niv,2014)。利用认知地图来解释潜伏学习实验，就好比说动物利用基于模型的算法，并且动物可以不依赖于外显的奖励或惩罚而学习到环境模型。动物受到奖励或惩罚从而具有动机的时候，可以用模型来进行规划。

托尔曼的认知地图说，具体来说就是动物通过探索环境过程中经历的一系列刺激，从而学习到刺激-刺激联结。在心理学当中这叫期望理论：如果存在刺激-刺激联结，那么当一个刺激出现时，个体就会预期另一个刺激会随后到来。这很类似于控制工程师所谓的系统识别，具有未知动力的系统模型，可以通过带有标识的训练集来加以学习。最简单的离散时间系统识别当中，训练集是S-S’对，S是一种状态，S’是随后的状态，是一种标识。当观察到S的时候，系统就会产生一种预期：随后将观察到S’。模型对于规划行为也有用，比如SA-S’对，当状态S下执行行动A，那么系统会预期S’随后发生。这对于学习环境如何产生奖励也很有用。比如S-R或SA-R，R可以看作伴随S或者SA对的奖励信号。这些都是监督学习的形式，智能体不管有没有接收到非零的奖励信号，都会在探索环境时获得认知地图。

14.7习惯性和目标指向性行为

模型无关和基于模型强化学习算法之间的区别对应于心理学家所指的习得行为模式的习惯性和目标指向性控制。习惯指适宜刺激所激发的行为模式，具有不同程度的自动化水平。目标指向性行为，根据心理学家的用法，指一种有目的的、由个体认识到的目标价值以及行动-后果之间关系所决定的行为。习惯有时受到其后果的控制（Dickinson, 1980, 1985）。目标指向性控制的优势在于，当环境对动物行动的反应改变之后，可以通过这类控制方式迅速改变动物的行为。而习惯性行为可以快速应对熟悉环境当中的刺激输入，然而却不能快速适应环境的改变。目标指向性行为控制的发生很可能是动物智能进化过程中的重要一步。

图14.9描述了模型无关和基于模型决策策略之间的差别，这种差别存在于假定的任务当中，一只老鼠穿越放置有多个目标盒子的迷宫，每个盒子都会发放一种奖励，奖励强度见图14.9上图。从S1开始，老鼠首先选择左侧或右侧路径，然后再次选择S2或S3的左侧或右侧路径，然后到达一个目标盒子。目标盒子可以看作老鼠多回合任务的结束状态。模型无关策略（图14.9左下方图）取决于状态-行动对的储存（缓存）价值。这些行动价值（Q值）是老鼠从每个状态当中采取每步行动的最高行动价值的估计值，是从多次试验当中获得的。当行动价值已经能够很好地估计最大返回值，老鼠只需要在每个状态下选择具有最高行动价值的行动，从而做出最优决策。这种情况下，当行动价值的估计值足够准确，老鼠会在S1处选择左侧路径，而在S2处选择右侧路径，从而达到最大返回值4。而模型无关的策略只取决于缓存策略而不是行动价值，也即是将S1和左侧路径，以及S2和右侧路径联系起来。在两种情况下决策都不需要环境模型。因此没有必要访问状态-转换模型，也不需要将目标盒子的特征和其所提供的奖励联系起来。

图14.9（右下方图）展示了基于模型的策略。这种策略使用了环境模型，包括状态-转换模型和奖励模型。状态-转换模型用决策树来表示，奖励模型将目标盒子的不同特征和每种特征下得到的奖励联系起来（和状态S1，S2，S3有关的奖励也是奖励模型的一部分，但这里这些奖励都为0，所以图中没有显示）。基于模型的智能体可以通过使用模型来决定每个状态下如何转向，这种模型可以模拟行动序列，从而找到可以输出最高返回值的那条路径。这种情况下返回值就是路径末端得到的奖励。比如在这个例子当中，如果模型足够准确，老鼠就会选择左侧路径然后右侧路径来获得奖励4。比较模拟路径的预期返回值是一种简单的规划，可以通过第八章介绍的一些方法来实现。

当模型无关智能体的环境对其行动的反馈发生变化，智能体就不得不学会新的经验，来更新策略和/或价值函数。在图14.9显示的模型无关策略当中，如果其中一个目标盒子开始发放不同的奖励，老鼠可能需要跨越迷宫很多次，从而在到达目标盒子的时候体验新的奖励，与此同时基于这种新的体验，更新策略或行动价值函数（或同时更新两者）。最关键的在于，为了使模型无关智能体改变某种状态下的行动策略，或改变和这种状态有关的行动价值，智能体需要移动到这种状态下，然后从这里开始行动，也许需要很多次之后才体验到相应行动的后果。

基于模型的智能体则不用亲身体验不同的状态和行动就可以适应环境变化。模型的变化会（通过规划）自动改变策略。规划可以决定环境变化的后果，哪怕这些变化之间的种种联系没有被智能体亲身体验过。比如，以图14.9当中的迷宫任务为例，想象一只老鼠之前学习到了转换和奖励模型，然后将这只老鼠放置到S2右侧的目标盒子前，它发现这时候获得的奖励值是1而不是4。老鼠的奖励模型就会变化，哪怕这只老鼠并没有发动到达目标盒子所必须的行动。规划过程会考虑到新奖励有关的知识，而不用老鼠在迷宫当中实际体验，根据规划老鼠会改变在S1和S3右转的策略，来获得奖励值3。

实际上这就是动物结果贬值任务的实验逻辑。这些实验结果可以帮助我们了解动物是否学会了某种习惯，或它的行为是不是在目标指向性控制之下发生的。结果贬值任务就像是潜伏学习任务一样，奖励变化到另外一个阶段。在第一个学习阶段获得奖励之后，某种结果的奖励价值发生了改变，变成了0或者负值。

第一个做这类实验的是Adams和Dickinson（1981）。他们通过操作性条件作用训练老鼠，直到老鼠在训练箱当中使劲地按压杠杆来获得糖丸。然后他们将老鼠放置在同一个箱子当中，这次杠杆被收回，老鼠可以无限制地食物作为奖励，意味着是否获得糖丸和老鼠的行动无关。15分钟后，给老鼠注射一种引起恶心症状的毒素氯化锂，在三次重复之后，没有老鼠再食用免费的糖丸，意味着糖丸的奖励价值下降了——糖丸贬值了。下一个阶段（一天后），再次将老鼠放到箱子当中，给予消退训练，杠杆再次放回原处，但是和发放糖丸的装置没有连接起来，因此按压是得不到糖丸的。研究者想要搞清楚的问题是，之前经历过奖励价值贬值的老鼠是否更少按压杠杆，即便按压杠杆不再经历贬值的奖励（即伴随恶心等症状）。结果发现从消退实验一开始，注射了毒素的老鼠就比没有接受注射的老鼠按压杠杆的次数更少。

Adams和Dickinson得出结论说，注射了毒素的老鼠首先通过将杠杆和糖丸联系起来获得了认知地图，之后又将按压杠杆和恶心症状联系起来。因此在消退实验当中，老鼠知道了按压杠杆的后果并非它们想要的，所以从一开始就减少了按压杠杆的次数。重要的一点是，它们不需要直接按压杠杆来体验那种恶心的感受，就直接降低了按压杠杆反应：实际上当它们感到恶心的时候并没有杠杆可以按压。老鼠看起来可以将行为选择结果的知识（按压杠杆会伴随获得糖丸），和结果的奖励价值（没有获得糖丸）联系起来，并相应地改变了它们的行为。不是每个心理学家都认可从认知角度来解释这个实验，并且这个解释也不是唯一可能的解释，但是用基于模型的规划来解释得到了很多心理学家的认可。

没有什么可以阻止一个智能体同时运用模型无关和基于模型的算法，并且使用两种策略理由充分。我们从自身经验中可知，只要重复在足够多次数，目标指向行为就会倾向于变成习惯性行为。实验表明对老鼠而言也是如此。Adams（1982）做了一个实验来看看是不是反复训练可以将目标指向行为变成习惯性行为。他比较了经历不同训练强度的老鼠结果贬值的效应值。如果相对于训练的少的老鼠，重复训练使得老鼠对贬值更不敏感了，那么这就证实了反复训练可以使行为习惯化。Adams的实验严格按照Adams和Dickinson（1981）的实验方法进行。简单描述下这个实验，一组老鼠训练到按压100次杠杆并获得奖励，另一组老鼠——过度训练组——按压杠杆500次并获得奖励。训练过后，两组老鼠都采用氯化锂注射法使得糖丸的奖励价值降低了。然后两组老鼠都给予消退训练。Adams想知道是否贬值会对过度训练老鼠按压杠杆的次数影响更小些，如果是这样，那就证实了过度训练可以降低结果贬值的敏感性。结果发现贬值的确降低了没有过度训练的老鼠按压杠杆的次数。与此相反，对过度训练的老鼠而言，贬值对按压杠杆次数的影响很小，事实上，这些老鼠按压的更起劲了。（完整实验方案还包括了控制组，也就是不同训练强度没有影响学习后的按压杠杆次数）。这个结果提示了，未过度训练的老鼠以一种目标指向性的方式来行动，对行动结果具有敏感的认识，而过度训练的老鼠则形成了按压杠杆的习惯。

从计算角度来理解这个实验结果，可以帮助我们认识到为何动物在一种情形下会表现出习惯性行为，而在另外一些情形下会表现出目标指向行为，还有为何当它们继续学习后会从一种行动控制方式转向另一种行动控制方式。尽管动物无疑不会完全运用我们在本书中介绍的算法来指导行动，但是我们可以考虑动物的行为是否可以通过具有不同权重的强化学习算法来理解。计算神经科学家Daw，Niv和Dayan（2005）提出一种观点，动物会使用模型无关和基于模型两种过程。每种过程可以选择一种行动，而选择执行的是根据学习过程中测量到的置信度判断为更可信的那种行动。在学习早期基于模型系统的规划更为可靠，可以将短期预测彼此联系起来，这时候由于个体经验较少，相对于模型无关过程需要对长期预测进行缓存，基于模型的规划结果更为准确。但是随着个体积累经验，模型无关过程渐渐变得更为可靠，因为由于模型的不准确性和出于可行性考虑选取捷径（例如树修剪），基于模型的规划很容易出错。根据这种思想，我们可以想见随着经验积累个体会逐渐从目标指向性行为转向习惯性行为。还有研究者提出观点说明动物是如何在目标指向和习惯性控制当中进行抉择，行为学和神经科学研究都在持续讨论这些问题。

对模型无关和基于模型算法的区分对研究是有帮助的。可以在抽象情境下考察这些算法的计算意义，因为这些情境可以反映出不同算法的优势和不足。而区分两类算法不仅提示有必要设计实验来增进心理学家对习惯性和目标指向性行为控制的认识，而且有助于明确如何设计此类实验。

14.8小结

本章的目的在于讨论强化学习和心理学当中动物、人类学习实验研究之间的对应关系。我们一开始就强调，强化学习并非用于模拟动物行为的细节，而是从人工智能和工程角度探讨理想情境的抽象的计算框架。但是许多基本的强化学习算法都受到了心理学理论的启发，有时候这些算法还推动了动物学习新模型的提出。这一章指出了二者之间最明显的对应关系。

用于预测和控制的强化学习算法之间的区别类似于经典/巴甫洛夫条件作用和工具性条件作用。工具性条件作用和经典条件作用实验的关键区别在于，前者强化刺激和动物行为是具有依随关系的，后者不是。通过TD算法学会预测，类似于经典条件作用，我们将经典条件作用的TD模型作为动物学习行为的强化学习原则的实例。通过纳入时间维度，描述一次试验内部的事件如何影响学习，该模型扩展了之前具有影响力的Rescorla-Wagner模型。TD模型还可以描述次级条件作用，这种情况下强化刺激的预测因素也变成了强化刺激。TD模型还是脑多巴胺神经元活动的基础，我们将在第十五章进行介绍。

通过尝试错误进行学习是强化学习控制论的基础。我们介绍了利用猫和其他动物进行的实验，利用这些实验桑代克提出了他的效果律，这部分内容本章和第一章都进行了讨论。我们指出，对于强化学习来说，探索不一定是盲人摸象；可以利用先天的或者先前学习到的知识、采用复杂方法进行尝试，只要有探索的成分在里面。我们讨论了斯金纳的行为塑造训练方法，通过逐渐改变奖励依随性，从而使得动物渐渐接近理想的行为。塑造不仅对于动物训练至关重要，而且还是训练强化学习智能体的有效工具。此外强化学习还和动物的动机状态有关，因为动机状态影响了动物是采取接近还是回避行为，也就是说事件对动物是奖励性质的还是惩罚性质的。

本书介绍的强化学习算法包括两种基本的解释延迟强化的机制：资格迹，以及通过TD算法学习到的价值函数。两种机制在之前的动物学习理论当中都有所涉及。资格迹类似于早期理论提到的刺激痕迹，价值函数对应于提供近乎即时评价反馈的次级强化。

还有一个对应之处是，强化学习的环境模型和心理学家所谓的认知地图。20世纪中叶进行的实验显示，动物不仅仅可以学会状态-动作关系，而且具有学习认知地图的能力。强化学习的环境模型就像是认知地图，可以通过监督学习方法、而不依赖于建立奖励信号进行学习，认知地图可以用于计划之后的行为。

强化学习对模型无关和基于模型算法的区分对应于心理学对习惯性和目标指向性行为的划分。模型无关算法通过采纳缓存在策略或者行动-价值函数当中的信息做出决策，而基于模型算法则通过使用智能体的环境模型进行提前规划，从而做出决策。结果贬值任务可以说明，动物行为是习惯性还是目标指向性控制的。强化学习理论可以帮助进一步阐明这些问题。

14.9结论

这一章我们讨论了和心理学动物学习理论相对应的一些强化学习概念和算法。动物学习显然可以帮助理解强化学习，但是作为一种机器学习类型，强化学习目的是用于设计和理解有效的学习算法，而不是重复或解释动物行为的细节。因此我们主要关注和解决预测、控制问题明显有关的动物学习理论，强调强化学习和心理学的观点可以彼此促进，但是不过多涉及行为细节以及动物学习研究的争议之处。如果将来动物学习特征的计算效用被更好地认识到，那么就可以深入联系这些特征来发展强化学习理论和算法。我们期待强化学习和心理学领域之间的交流能够促进两个学科产生更多成果。

14.10参考书目和历史评述

Ludvig，Bellemare和Pearson（2011）和Shah（2012）回顾了心理学和神经科学背景下的强化学习历史。这些综述和本章内容以及下一章可以互为参考。

14.1据我们所知，动物学习领域最早提出强化这一术语是在1927年，出现于巴甫洛夫著作的英文版译本当中（Pavlov, 1927），该书将强化这一词用于非行动依随性的情形。Mackintosh（1983）建议用强化来指代增强或者减弱某种行为模式。Skinner（1983，1963）提出用强化增强行为模式，而用惩罚来表示减弱行为模式。奖励和奖励信号之间的区别来自于Schultz（2007a，2007b）.Dickinson（1985）讨论了反应和行动之间的区别。

14.3Rescorla-Wagner模型的思想是当动物感到惊讶，就发生了学习，这种思想来源于Kamin（1969），Kamin第一次报告了经典条件作用下的阻滞效应，即现在广为人知的Kamin阻滞（Kamin, 1968）。Moore和Schmajuk（2008）非常好地概述了阻滞现象，该现象引发的一系列研究，以及对动物学习理论的持续影响。除Rescorla-Wagner模型之外，其他经典条件作用模型包括Klopf（1988），Grossberg（1975），Mackintosh（1975），Moore和Stickney（1980），Pearce和Hall(1980)，Courville, Daw和Touretzky (2006)。Schmajuk（2008）回顾了经典条件作用模型。

经典条件作用的TD模型早期版本见于Sutton和Barto（1981），预测了时间接近程度大于组织效应的作用，后来Kehoee，Scheurs和Graham（1987）在兔子瞬膜条件作用实验当中得到证实。Sutton和Barto（1981）认识到Rescorla-Wagner模型和最小均方法（Least-Mean-Square, LMS）以及Widrow-Hoff学习原则之间的关系 (Widrow and Hoﬀ, 1960)。在Sutton开发了TD算法(Sutton, 1984,1988) 之后修改了TD早期模型，并在1987年第一次提出TD模型，在1990年提出更为完整的模型。本节内容大部分基于该模型。Moore及其同事更进一步探讨了TD模型及其神经执行 (Moore, Desmond, Berthier, Blazis, Sutton, andBarto, 1986; Moore and Blazis, 1989; Moore, Choi, and Brunzell, 1998; Moore,Marks, Castagna, and Polewan, 2001)。Klopf的经典条件作用驱力强化理论扩展了TD模型，能够解释更多实验细节，例如S型学习曲线。在TD的一些论著当中，TD意指时间导数（Time Derivative）而不是时序差分（TemporalDiﬀerence）。Ludvig，Sutton和Kehoe（2012）评价了TD模型在以前未探讨的经典条件反应等任务当中的表现，并考察了不同刺激表征的影响，包括他们之前介绍过的微刺激表征(Ludvig, Sutton, and Kehoe, 2008)。Moore及其同事较早在TD框架下考察了不同刺激表征对反应时间和反应脑区的影响，及其可能的神经执行。尽管Ludvig等（2012）对于微刺激表征的研究不在TD模型框架下，然而之后 Grossberg和Schmajuk (1989)，Brown、Bullock和Grossberg(1999)，Buhusi和Schmajuk (1999)，以及Machado (1997)等都对此进行了研究并提出相应观点。

14.4 1.7节包括尝试错误学习和效果律的研究历史评述。Selfridge、Sutton和Barto描述了在平衡杆强化学习任务当中塑造的有效性。其他强化学习领域当中关于塑造的例子包括 Gullapalli和Barto (1992)，Mahadevan和Connell(1992)，Mataric (1994)，Dorigo和Colombette(1994)，Saksida、Raymond和Touretzky(1997)，Randløv和Alstrøm (1998)。 Ng (2003)，Ng、Harada和Russell(1999) 两篇文献当中，对塑造使用了和斯金纳不同的定义，用来指如何不改变最优策略来改变奖励信号。Dickinson和Balleine (2002)讨论了学习和动机之间复杂的交互作用。Wise（2004）概括了强化学习和动机之间的关系。Daw和Shohamy（2008）将动机、学习和强化学习理论的对应方面联系起来。另外请参考McClure、Daw和Montague(2003)，Niv、Joel和Dayan (2006)，Rangel等(2008)，以及Dayan和Berridge (2014)的著作。McClure等(2003)，Niv、Daw和Dayan(2005)，以及Niv、Daw、Joel和Dayan(2007) 也联系强化学习框架提出了行为活力（vigor）理论。

14.5Hull的学生及其在耶鲁的同事，详述了延迟强化问题当中次级强化的作用(Spence, 1947)。在间隔非常长的延迟条件下学习，例如味觉厌恶条件作用间隔可达数小时，这种情况下就会遵循干扰理论而不是延迟痕迹理论(比如Revusky和Garcia, 1970，Boakes和Costa,2014) 。其他延迟强化的学习观点也启发研究者认识到觉醒和工作记忆的作用 (比如Clark和Squire, 1998；Seo、Barraclough和Lee,2007)。

14.6Thistlethwaite（1951）发表了一篇当时看来最详尽的关于潜伏学习实验的综述。Ljung（1998）则回顾了模型学习，或系统识别，工程技术方面的综述。 Gopnik、Glymour、Sobel、Schulz、Kushnir和Danks(2004)提出了关于儿童如何学习模型的贝叶斯理论。

14.7习惯性和目标指向性行为之间的联系，以及模型无关和基于模型强化学习之间的联系，第一次由Daw，Niv和Dayan（2005）提出。他们假设了一种迷宫任务，用来解释习惯性和目标指向性行为控制（Niv, Joel和Dayan, 2006）。Dolan和Dayan（2013）回顾了和这类问题有关的四种实验研究传统，讨论了如何利用强化学习模型无关/基于模型的划分方法来推动该领域研究。Donahoe和Burgos (2000)则主张模型无关过程可以解释结果贬值任务的结果。Dayan和Berridge (2014)认为经典条件作用也包含了基于模型的过程。Rangel、Camerer和Montague(2008)回顾了许多尚未解决的包含习惯性、目标指向性和巴甫洛夫控制的问题。

Fig14.9用基于模型和模型无关策略解决假设的序列行动选择问题。上图：一只老鼠经过迷宫达到不同的目标箱子，每只箱子有着不同的奖励价值。左下图：模型无关策略依靠的是存储的多次学习当中所有状态-行动对的（缓存）行动价值。为了做出决策老鼠需要在每种状态下选择当下具有最大行动价值的行动。右下图：基于模型策略指的是老鼠需要学习环境模型，包括对状态-行动-下次状态转换的知识，以及和每种目标相伴随的奖励模型。老鼠需要决定每种状态下如何利用模拟行动序列的模型改变方向，选择具有最高回报的路径。改编自认知科学趋势，第10卷，第八册， Y. Niv, D. Joel, &P. Dayan，动机的规范视角， p. 376,

2006,经Elsevier许可.

------

[[1\]](#_ftnref1) 这和Sutton和Barto（1990）的CSC表征不同，每一时刻的独立特征是相同的，但是未提及外部刺激，因此在名称上补充了序列复合刺激。