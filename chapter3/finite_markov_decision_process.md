### 第三章

### 有限马尔科夫决策过程

​        在本章中，我们将介绍我们在本书其余部分中尝试解决的问题。这个问题可以被认为定义强化学习的领域：任何适合于解决这个问题的方法，我们认为是强化学习方法。

​        在这章我们的目标是在广义上来描述强化学习。我们试图传达可能被用作强化学习任务的各种可能的应用程序。我们还描述了可以进行精确的理论陈述的强化学习问题的理想化数学形式。我们介绍该问题的数学结构的关键部分，例如价值函数和贝尔曼方程。在所有的人工智能中，在适用性的宽度和数学易处理性之间存在矛盾（a tension）。在本章中，我们将介绍这种矛盾，并讨论一些它所带来的权衡和挑战。

####         3.1智能体环境接口

​        强化学习问题意味着从交互学习到实现目标的问题的直接框架。智能体在其中进行学习和决策。除了智能体本身，智能体与之交互的一切，称之为环境。它们不断地交互，智能体选择动作同时环境响应那些动作并向智能体呈现新的情况[^1，智能体试图随着时间最大化环境所反馈的奖励数值。一个完整规范的环境，包括如何确定奖励，定义一个强化学习问题的实例作为任务。

​                                ![pic3.1](pic3.1.jpg)     

######                                                                                                  <center>图3.1：强化学习中智能体与环境交互图</center>

​         具体来说，智能体和环境在离散时间序列每一步相互作用，t = 0,1,2,3,....[^2] 在每一步t，智能体感应到环境状态，$S_{t}\in \mathcal{S}$,$\mathcal{S}$是所有可能状态的集合，并在此基础上选择动作，$A_{t} \in\mathcal{A}(S_{t}) $  ，$\mathcal{A}(S_{t})$是在状态$S_{t}$上所有动作的集合。每一步后，作为它行动的结果，智能体接收到一个奖励值，$R_{t+1} \in \mathcal{R} \subset  \mathbb{R}$，并且自身处于一个新状态，$S_{t+1}$ [^3] ，图3.1展示了智能体- 环境交互过程。

​        在每一步，智能体实现从状态到选择每个可能动作的概率的映射。 此映射称为代理的策略，并表示为$\pi _{t}$，当$S_{t} =s$时，$A_{t}=a$的概率是$\pi _{t}(a|s)$，强化学习方法指定智能体根据经验改变其策略的方式。智能体的目标，大致来说，是最大化的获得它的长期总收益。

​      这个框架是灵活而抽象的，可以以不同的方式应用在很多不同的问题上。例如，时间步长不需要参考实时的固定间隔；它们可以指任意连续的决策和行动阶段。动作可以是低级控制，例如施加到机器人臂的电机的电压，或者高级决策，例如是否吃午饭或去研究生院。类似地，状态也可以采取各种各样的形式。它们可以完全由低级感应（例如直接传感器读数）来确定，或者它们可以是更高级的和抽象的，例如房间里的物体符号表示。可以基于对过去的感觉的记忆，甚至是完全精神的或主观的来构成一个状态。例如，代理可能处于不能确定物体在哪里，或者在某种明确定义的意义上感到惊讶。类似地，一些动作可以是完全精神的或可计算的。例如，某些操作可能会控制智能体选择思考的内容，或者注意力集中在哪里。一般来说，动作可以是我们想要学习如何做的任何决定，而状态可以是任何我们可以知道，可能有用的。

​       特别的，智能体和环境之间的边界通常不像机器人或动物身体的物理边界。通常，边界更接近于智能体。例如，机器人及其感测硬件的电动机和机械联动件通常应被认为是环境的部分而不是智能体的部分。同样，如果我们将框架应用于人或动物，肌肉，骨骼和感觉器官应被视为环境的一部分。奖励也可以在自然和人工学习系统的物理体内计算，但被认为是智能体外部的。

​    

​        

[^1]: 我们使用术语智能体，环境和动作，而不是工程师术语控制器，受控系统（或工厂）和控制信号，因为它们对更广泛的受众有意义。

[^2]: 我们将注意力限制在离散时间以使事情尽可能简单，即使许多想法可以延伸到连续时间情况（例如，参见Bertsekas和Tsitsiklis，1996; Werbos，1992; Doya，1996）。
[^3]: 我们使用$R_{t+1}$而不是$R_{t}​$来表示归因于$A_{t}​$的奖励，因为它强调下一个奖励和下一个状态$R_{t+1}​$和 $S_{t+1}​$共同确定。 不幸的是，这两种惯例在文献中都被广泛使用。