### 第三章

### 有限马尔科夫决策过程

​        在本章中，我们将介绍我们在本书其余部分中尝试解决的问题。这个问题可以被认为定义强化学习的领域：任何适合于解决这个问题的方法，我们认为是强化学习方法。

​        在这章我们的目标是在广义上来描述强化学习。我们试图传达可能被用作强化学习任务的各种可能的应用程序。我们还描述了可以进行精确的理论陈述的强化学习问题的理想化数学形式。我们介绍该问题的数学结构的关键部分，例如价值函数和贝尔曼方程。在所有的人工智能中，在适用性的宽度和数学易处理性之间存在矛盾（a tension）。在本章中，我们将介绍这种矛盾，并讨论一些它所带来的权衡和挑战。

####         3.1智能体环境接口

​        强化学习问题意味着从交互学习到实现目标的问题的直接框架。智能体在其中进行学习和决策。除了智能体本身，智能体与之交互的一切，称之为环境。它们不断地交互，智能体选择动作同时环境响应那些动作并向智能体呈现新的情况[^1]	，智能体试图随着时间最大化环境所反馈的奖励数值。一个完整规范的环境，包括如何确定奖励，定义一个强化学习问题的实例作为任务。

​                                 ![20170221132442](C:\Users\Administrator\Desktop\20170221132442.png)

######                                                                图3.1：强化学习中智能体与环境交互图

​         具体来说，智能体和环境在离散时间序列每一步相互作用，t = 0,1,2,3,....[^2] 在每一步t，智能体感应到环境状态，$S_{t}\in \mathcal{S}$,$\mathcal{S}$是所有可能状态的集合，并在此基础上选择动作，$A_{t} \in\mathcal{A}(S_{t}) $  ，$\mathcal{A}(S_{t})$是在状态$S_{t}$上所有动作的集合。

[^1]: 我们使用术语智能体，环境和动作，而不是工程师术语控制器，受控系统（或工厂）和控制信号，因为它们对更广泛的受众有意义。

[^2]: 我们将注意力限制在离散时间以使事情尽可能简单，即使许多想法可以延伸到连续时间情况（例如，参见Bertsekas和Tsitsiklis，1996; Werbos，1992; Doya，1996）。