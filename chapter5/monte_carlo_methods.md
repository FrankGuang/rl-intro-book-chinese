# 第五章  

# 蒙特卡洛方法

​	这一章，我们开始考虑评估值函数以及获得最优策略的第一种学习方法。不同于上一章，这里我们不再假设我们对环境有完全的了解。蒙特卡洛方法(Monte Carlo methods)需要的仅仅是*经验(experience)*——与环境进行真实的或者模拟的交互所得到的状态，动作，奖励的样本序列。其中，从*真实*的经验学习是非常吸引人的，因为它在不需要关于环境动态的先验知识的情况下仍然能够获得最优的行为（策略）；而从*模拟*的经验学习也同样强大，虽然这时需要一个模型，但是这个模型仅仅用来产生样本，并不是动态规划(DP)方法中所用到的所有转移概率的完整分布函数。在相当多情况下我们很容易从目标的概率分布函数中进行抽样得到样本，可是很难获得这个分布的显式（具体）形式。[^译者注1]

​		

[^译者注1]: 这里是说，很多情况下我们不知道转移概率的具体分布，所以很难用动态规划的办法。但是我们很容易从与环境交互中获得抽样样本，可以用蒙特卡罗的办法。因为样本是直接从环境中获得的，等效于直接从真实的转移概率分布中抽样。

​	蒙特卡罗方法是基于对样本回报(return)求平均的办法来解决强化学习的问题的。为了保证能够得到良好定义的回报，这里我们定义蒙特卡洛方法仅适用于shijianshi的任务。就是说，我们假设我们的经验(experience)被分成一个个的事件，而且对每个事件而言，不管选择什么样的动作，都会结束。只有在事件结束时，我们的价值估计和策略才会改变。蒙特卡洛方法因此能够写成逐个事件的增量形式。不过不能写成逐步（在线）的形式。术语“蒙特卡洛”被广泛的用于任何的在操作中引入了随机成分的估计方法。这里我们使用它来表示基于平均整个回报的方法（区别于那些使用部分的回报的方法。我们将在下一章阐述）。

​	蒙特卡洛方法使用抽样以及对状态-动作对的*回报(return)*求平均的办法很像我们在第二章中遇到的摇臂机中使用的方法，在第二章中我们也使用了抽样以及对每个动作的*奖励*求平均的方法。他们主要的区别在于，我们现在有多种状态，每个表现地就像一个不同的摇臂机问题（就像一个联合-搜索或前后关联的摇臂机），而且它们之间是相互关联的。就是说，在一个状态下做出一个动作的回报依赖于本事件中这个状态之后的状态所做的动作。因为所有动作的选择也正在学习中，从之前的表述来看，问题变成了非平稳(nonstationary)的。

​	为了解决这种非平稳性(nonstationarity)，我们改变我们的办法，像我们在第四章中对动态规划方法(DP)所做的，使用广义策略迭代(general policy iteration, GPI)。那里我们依靠对MDP的了解来*计算(computed)*价值函数(value function)，这里呢我们从MDP的抽样回报中*学习(learn)*价值函数(value function)。我们使用相同的办法去获得最优的价值函数和策略，即GPI中价值函数和对应的策略交互作用。就像在动态规划(DP)的那章所做的，首先我们考虑yuce(prediction)的问题（计算一个随机策略$\pi$的价值$v_{\pi}$和$q_{\pi}$），然后是策略改进(policy improvment)，以及，最后，kongzhi(control)的问题和解决它的办法广义策略迭代(general policy iteration, GPI)。从动态规划(DP)中得到的这些想法都被推广到蒙特卡洛(Monte Carlo)方法中。在这种情况下，我们只有样本经验(sample experience)。



---



## 5.1 蒙特卡洛预测



---



## 5.2 对动作价值的蒙特卡洛估计



---



## 5.3 蒙特卡洛控制



---



## 5.4 非初始探索的蒙特卡洛控制



---



## 5.5 通过重要性采样的离策略预测



---



## 5.6 增量式的实现



---



## 5.7 离策略控制



---



## *5.8 具体返回的重要性采样



---



## 5.9 小结



---



 

