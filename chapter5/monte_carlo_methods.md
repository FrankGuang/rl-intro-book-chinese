# 第五章  

# 蒙特卡洛方法

​	这一章，我们开始考虑评估值函数以及获得最优策略的第一种学习方法。不同于上一章，这里我们不再假设我们对环境有完全的了解。蒙特卡洛方法(Monte Carlo methods)需要的仅仅是*经验(experience)*——与环境进行真实的或者模拟的交互所得到的状态，动作，奖励的样本序列。其中，从*真实*的经验学习是非常吸引人的。因为它在不需要关于环境动态的先验知识的情况下仍然能够获得最优的行为（策略）；而从*模拟*的经验学习也同样强大。虽然这时需要一个模型，但是这个模型仅仅用来产生样本，并不是动态规划(DP)方法中所用到的所有转移概率的完整分布函数。在相当多情况下我们很容易从目标的概率分布函数中进行抽样得到样本，可是很难获得这个分布的显式（具体）形式。[^译者注1]

[^译者注1]: 这里是说，很多情况下我们不知道转移概率的具体分布，所以很难用动态规划的办法。但是我们很容易从与环境交互中获得抽样样本，可以用蒙特卡罗的办法。因为样本是直接从环境中获得的，等效于直接从真实的转移概率分布中抽样。

​	



---



## 5.1 蒙特卡洛预测



---



## 5.2 对动作价值的蒙特卡洛估计



---



## 5.3 蒙特卡洛控制



---



## 5.4 非初始探索的蒙特卡洛控制



---



## 5.5 通过重要性采样的离策略预测



---



## 5.6 增量式的实现



---



## 5.7 离策略控制



---



## *5.8 具体返回的重要性采样



---



## 5.9 小结



---



 

