# 第五章  

# 蒙特卡洛方法

​	这一章，我们开始考虑评估值函数以及获得最优策略的第一种学习方法。不同于上一章，这里我们不再假设我们对环境有完全的了解。蒙特卡洛方法(Monte Carlo methods)需要的仅仅是*经验(experience)*——与环境进行真实的或者模拟的交互所得到的状态，动作，奖励的样本序列。其中，从*真实*的经验学习是非常吸引人的，因为它在不需要关于环境动态的先验知识的情况下仍然能够获得最优的行为（策略）；而从*模拟*的经验学习也同样强大，虽然这时需要一个模型，但是这个模型仅仅用来产生样本，并不是动态规划(DP)方法中所用到的所有转移概率的完整分布函数。在相当多情况下我们很容易从目标的概率分布函数中进行抽样得到样本，可是很难获得这个分布的显式（具体）形式。[^译者注1]

​		

[^译者注1]: 这里是说，很多情况下我们不知道转移概率的具体分布，所以很难用动态规划的办法。但是我们很容易从与环境交互中获得抽样样本，可以用蒙特卡罗的办法。因为样本是直接从环境中获得的，等效于直接从真实的转移概率分布中抽样。

​	蒙特卡罗方法是基于对样本返回值求平均的办法来解决强化学习的问题的。为了保证能够得到良好定义的返回值，这里我们定义蒙特卡洛方法仅适用于shijianshi的任务。就是说，我们假设我们的经验(experience)被分成一个个的事件，而且对每个事件而言，不管选择什么样的动作，都会结束。只有在事件结束时，我们的价值估计和策略才会改变。蒙特卡洛方法因此能够写成逐个事件的增量形式。不过不能写成逐步（在线）的形式。术语“蒙特卡洛”被广泛的用于任何的在操作中引入了随机成分的估计方法。这里我们使用它来表示基于平均整个返回值的方法（区别于那些使用部分的返回值的方法。我们将在下一章阐述）。





---



## 5.1 蒙特卡洛预测



---



## 5.2 对动作价值的蒙特卡洛估计



---



## 5.3 蒙特卡洛控制



---



## 5.4 非初始探索的蒙特卡洛控制



---



## 5.5 通过重要性采样的离策略预测



---



## 5.6 增量式的实现



---



## 5.7 离策略控制



---



## *5.8 具体返回的重要性采样



---



## 5.9 小结



---



 

