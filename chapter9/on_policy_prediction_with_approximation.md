第9章

模拟在策略（On-policy）的预测



本章，我们将开始探究如何在强化学习中用函数拟合（function approximation）从在策略数据中估计状态值函数。即，从已知策略$\pi$产生的经验中去估计xxx。本章的亮点在于用于估值的函数不再是一张表，而是权重向量xxx来表示的。记xxx为给定权重向量θ下状态s的估值。比如，xx可以是一个用特征向量xx来表示状态的线性方程。更一般地，xxx可以是个用多层人工神经网络计算的方程，其中θ向量作为连接各层权重的参数。通过调整权重参数，任何方程都可通过神经网络来模拟（implement？）。又或者xxx可以是个决策树计算的方程，其中θ代表决策树中分流点和叶节点的值。一般来说，权重的数量（θ集合中的成员数）要远小于状态数（xxx），而更新一个权重会影响(change)很多状态的估值。(这种估值方法的)结果是，当仅仅更新了一个状态后，因为该状态而衍生的变化会影响很多其他状态的估值。这种衍生特性潜在地使估值学习变得更强大而也变得更加难懂。

9.1 值函数(Value-function)估计

本书所言的所有预测方法都被描述为备份集（backups），换言之，所谓更新估值函数就是用“备份集中的值”来对更新某个状态值。记某个元素的备份为xxx,其中s代表被备份的状态而g代表被备份的值，也可以说是S更新后的目标值（target）。比如，蒙特卡洛法中的值预测备份为 xxx，TD(0)法中的备份为xxx，而n-stepTD备份法中的备份为xxx。在DP（动态规划，dynamic programming）策略估值备份则表示为xx，在实际值更新过程中，不是某个s被备份就是xx被备份了。

将每一份备份（backup）用作寻找值函数的输入-输出规律（behavior）是很自然的。 某种意义上，备份xxx 意味着需被预估的s值应该更像g才对。 到目前为止，实际执行的备份都是很容易的：估值表（table）中的s在一定程度上向g的方向更新了一点，而其他的状态估值并没有发生变化。 而现在，我们允许使用任意复杂难以理解的方法来备份。这种方法下对S的值更新也会随之更新其他状态的值。 用以模拟样本中输入-输出规律的机器学习方法称之为“监督学习”，而当输出是数字时，比如g，整个（模拟）过程往往被称为“函数拟合”。 函数拟合通过接收具有期望行为（规律）的输入-输出样本来进行估值。 通过简单地将每一份备份样本xxx作为训练样本传递给上述方法来进行值预测，然后我们将随之产生的估值函数用作（状态值）的估计。



将每份备份视作约定的训练样本使得我们可以使用各种现存的函数回归方法来进行值预测。基本上，通过样本我们可以使用任何监督学习的方法，包括人工神经网络，决策树，以及多种多样的多变量回归（multivariate regression）。然而，并非所有函数拟合方法都能很好地适用于强化学习。最玄妙的神经网络和静态方法都需要先假设有一个静态训练集，即在该假设下多份样本默认是被同时传入学习方程中的。而在强化学习中，做到在和环境（或其模型）交互中“在线”学习是很重要的。要做到这点，需要确保通过逐步获得的数据来有效地进行学习。另外，强化学习使用的函数拟合方法一般需要能处理非平稳目标方程（目标方程随时间而变动）。比如，在使用基于GPI()的控制方法时，我们往往需要在π变动的情况下学习xxx。即使策略表示不变，由bootstrapping法（DP和TD法）产生的训练样本目标值也是不平稳的。那些无法有效处理非平稳数据的方法就是不适用于强化学习了。

9.2 预测目标（均方误差-MSVE）

目前为止我们尚未明确指出用于预测的目标。在使用表格法的案例中，由于学习到的估值函数确实可以被当做真实的值函数，所以对其预测质量的持续估计是没有必要的。而且，学习到的值是彼此分离的---即对任意状态值得更新不会影响到影响到其他状态值。 但在真正的估计法中，对某状态的更新会影响到需要其他状态，因此（and）无法保证所有states估值绝对正确。基于假设，由于有比权重元素多得多的状态元素，因此精确化某状态的估值意味着误差化其他状态的估值。那么在这种情况下，我们有义务说明哪些状态我们更加关心。我们必须确定一个权重（weighting）或分布（distribution）xx来表示对于每个状态s误差的关心程度。 这里指的误差（error）是估值xx和真值xx的差的平方。通过分布d在状态空间中对误差进行衡量，于是我们很自然地得到了一个目标方程，“均方误差方程”（ the Mean Squared Value Error）---MSVE。

xxxxxxxxxx

对均方误差开方，即得均方误差根---RMSVE,其给出估值偏离真值程度的一个大概估计，同时其也经常被用于作图。基于目标策略π，我们一般取状态s下所用的时间的分数作为xxx。 这是所谓的“在策略分布”（ on-policy distribution）；本章我们着重全面地分析该情况。在连续任务中，在策略分布就是策略π下的平稳分布。



图xxxxxxxxxxxxxxxx

在情景任务(episodic tasks)中的在策略分布

在一个情景任务中，在策略分布略有不同，原因是它取决于每个片段的初始状态选择。记xxx为片段初始化于状态s的概率。同时记xxx为平均每个片段s所步过（experience）的总时间步长（time steps）。当片段初始化于状态s，又或者已步过的状态xx转移到状态s，我们就说状态s已步过：

 xxx公式（9.2）

通过访问xxx的期望值可以解决该方程，而通过xxx的正则化使之和为1来得到在策略的分布：

xxx （9.3）

图xxxxxxxxxxxxxxxx



持续性任务和片段式任务，尽管行为类似，但我们将在本书的这一部分中反复看到，在使用模拟的方法的正式分析中，他们必须被区别对待。而这种（分类）也完善了学习规范的学习目的。

在强化学习中，我们尚不完全清楚MSVE是否就是正确的性能衡量标的（object）。记得我们使用估值函数的终极目标在于通过其找到一个更好的策略。所以在该目标下，最好的估值函数未必需要最下化MSVE.然而，目前在尚不清楚是否有更好的选择下，我们将专注于MSVE。



基于MSVE，理想目标是找到一个“全局最优解”，即对所有xxx有xxx≤xxx。对于一些简单地函数比如线性方程之类的，这个目标可能做到的。但对于诸如人工神经网络和决策树这类复杂的函数模拟器则不太可能。这类复杂函数模拟器的缺点在于，相较于全局最优，其最后会收敛至“局部最优”。对xxx某个领域，有xxx≤xxx。尽管这种保证不够强有力，但对于非线性函数模拟器，这往往已是最好的，并且足够了。但在许多强化学习案例中，我们依然无法保证收敛至最有，哪怕是收敛至最优解的某个固定邻域内。 实际上，某些方法（反而）可能会发散，即在极限中他们的MSVE趋向于无穷。



在前面2章中，通过讨论大量以前序备份集产生供后续使用的训练样本的函数拟合方法，以及大量用以估值预测的强化学习方法，我们整合了一个框架。 为了最小化（估值）偏差，我们也引入了MSVE性能指标。 鉴于可用的函数拟合法实在太多了，而对于大部分方法如何给出一可靠的估计和推荐我们也知之甚少。所以出于必要性，我们仅考虑有限的几个方法（possibilities）。在本章后续部分，我们将专注于基于梯度原理的函数拟合方法，尤其是线性梯度下降法。 之所以重点讨论这些方法是因为我们认为他们是最有前景的方法。也因为这些方法揭示了一些关键原理。当然也因为我们篇幅有限而这些方法也足够简单。



9.3 随机梯度和半梯度法

在众多用以估值预测的学习方法中，现在我们开始深入探究基于随机梯度下降（SGD）的函数拟合方法。在所有函数拟合法中，SGD是最为广泛使用的方法，并且其很适合用于在线强化学习。



梯度下降法中的权重向量是一个由固定数量的元素集合xxx组成的列向量，而股值函数xxx这是一个对所有定义域在xxx的xxx可微函数。由于在每个一系列的离散时步，t=0,1,2,3....$\theta$会得到更新。所以我们需要定义xxx为每一时步中的权重向量。现在假设，每一时步中，我们观察到了一个新的样本xxx，其有（可能是随机选择的）状态xxx和其在该策略下的真值。基于环境互动的这些状态又可以是连续的，不过在此我们先不做该假设。即便针对每一个xxx能给出一个绝对正确的值xxx，但鉴于我们的函数模拟器受制于有限的资源也因此导致的有限的解决方案，函数拟合不是件简单的事情。尤其是在多数情况下，没有xxx能触及所有的状态，甚至是所有绝对正确的样本。另外，必须通过函数拟合来泛化出从没在样本中出现的状态。



假设所有样本中的状态均服从同一分布，d，在该假设下我们试图最小化9.1中给出的MSVE。一个好主意是从已经观察到的样本中尚持最小化误差。“随机梯度下降法”（SGD）在经历了每一份样本后，通过将权重向量往能最小化该样本误差的方向挪动一小步来更新权重：



xxxxxx （9,。4）

xxxxxx（9,。5）



式中xx是个正步长参数，而对于任意标量表达式xxx，xxx代表权重向量中所有元素的偏导数向量：



xxxxx （9.6）

该导数向量是f相对于θ的梯度。SGD之所以被称为“梯度下降”是因为在所有时步中，xxx正比例于样本方差的负梯度（9.4）。在这样的方向下？ 误差可以降低得最快。而称其为“随机”梯度法是由于仅有一个（随机抽取的）样本会被用来更新。在许多样本中（权重向量）挪动一小步，那么最后会达到最小化诸如MSVE的平均性能指标的总体效果。



可能无法马上明白为何SGD仅往梯度方向挪动一小步。（在最小化误差时，）我们是否可以不全都往该方向更新梯度，是否可以完全忽略某样本中的误差？很多时候这样做可行，但我们不推荐。 记住我们并不需要找到或期待一个误差为0的估值函数，而只需要该函数可以权衡所有不同状态的误差。若在某一时步中做到对每个样本的状态估值都绝对正确，那么我们将无法找到这种平衡。事实上，SGD的最终收敛需要假设xx在不断地递减。若α正如标准随机估计条件（2.7）那样递减，则SGD法(9.5)课最终保证收敛至局部最优。



现在来看这样一个例子。该例中xxx作为第t个训练样本的目标输出，xxx，不是真值xxx，而是可能是某个随机生成的模拟值。比如，xx可以是个有噪声的xx，或者是又前面章节提到的xx生成的bootstrapping目标中的一个。在这些案例中因为xxx是未知的，所以无法直接使用式（9.5）来更新梯度。步过我可以用xxx来代替xxx来进行估计。 这个想法引申出了如下用于状态值预测的一般SGD方法：

xxxxx（9.7）



图xxxxxxxxxxxxxxxx

图xxxxxxxxxxxxxxxx



如果xxx是无偏估计，即对于任意t，若有xxx=xxx，则基于递减α的通常随机近似条件（2.7）下，xxx能保证收敛至局部最优。

​	比如，若样本中的状态均是在策略xxx下通过与环境（模拟）互动产生的，则状态的真值就是其在该片段下随后得到的期望值。也就是说基于xxx的无偏估计定义，可以认为xxx=xxx。而有了这个选择后，对于xxx的值估计，就可以说一般SGD方法（9.7）是可以收敛至局部最优的了。因此，梯度下降版的蒙特卡罗的值估计法是可以保证找到一个局部最优解的。相应的伪代码已展示在框中。

​      如果目标xxx是由xxx的bootstrapping估计来表示的，则得不到同样的保证。诸如n步xxx或者DP xxxx的Bootstrapping目标值均依然于当前的权重向量xxx。其暗示了这种梯度下降是带有偏见的，所以基于这类目标值就不能执行一个真正意义上的梯度下降法。 （对于该结论的）一个解释可以看式（9.4） 到（9.5）的转化这一关键步骤。其中目标值是独立于xxx的。当用bootstrapping法代替了原式中的xxx，这一步骤将不再有效bootstrapping法实际上不是梯度下降的真正实例（Barnard,1993）。这种方法在估值时考虑了权重向量xxx的变化影响，却有意忽略了权重向量（its）对目标值的影响。可见这类方法只包含了部分梯度。鉴于此，我们称其为半梯度法（ semi-gradient methods）。

​	虽然半梯度（bootstrapping）法无法像一般梯度法那样稳定收敛。但正如下节所述的线性案例那样，在诸如这类主要案例中其确实可以有效收敛。而且这类方法也确实因为一些优势而受到偏爱。其一是这类方法往往学习得更快，正如6,7章所述。其二是它们可以不用等到片段结束，直接在线持续性的学习。这个特性使得该方法适用于连续型问题并借此发挥计算优势。一个使用半梯度法的原型是半梯度TD(0)法，其目标值为xxx。下一页顶部框中展示了该方法的完整伪代码。

​	

示例 9.1：从一个由1000个状态组成的随机步行任务中整合状态

​	“状态整合”是一般函数拟合的一个简单形式。其将状态集分组，用一个值（权重向量中的一个元素）来表示每一组状态的估值。某一状态的估值就是其所属组别的元素的值。当该状态值被更新时，代表该组的元素也单独（相较于其他权重向量元素）被更新。状态整合是SGD(9.7)的一个特殊案例，其中梯度，xxx，为1代表状态xxx组的元素，为0代表其他组别的元素。

​	考虑有100个状态的随机步行任务（示例6.2和7.1）。所有状态标示为1~1000，从左至右。所有回合初始化于中点附近，即状态500。从当前状态转移至其左右相邻的100个状态中的一个状态，称为一次状态转移。所有转移概率均等。当然，如果当前状态接近边缘，则该方向的状态集可能少于100个。这种情况下，那些丢失的状态概率被转移到了终止(terminating)状态（因此，状态1有0.5的概率在其左侧终止，而状态950有0.25的概率在其右侧终止）。按照惯例，该任务中的左侧终止状态的奖励为-1，而右侧的为+1。其他所有状态的转移奖励为0。我们将该步行任务作为示例将贯穿本节。

​	图9.1展示了该任务的真值函数xxx图。其接近于一条直线，并在水平方向略显弯曲，尤其是在（图上）最后100个状态里。图中也显示了由α=xxx的学习率和带状态整合的蒙特卡罗算法在经历了100,000个片段后学习到的最终状态估值。所谓状态整合，本例中是指将1000个状态分成10组，每组100个状态（举例：状态1-100在组1，状态101-200在另一组，以此类推）。图中所示的阶梯图形是典型的状态整合图；（可见）每一组中的估值都是恒定的，而当进入另一组时值会突然改变。这些估值接近于MSVE（9.1）全局最小值。



图xxxxxxxxxxxxxxxx

图xxxxxxxxxxxxxxxx

​	在上图中分布d就是图下半区域带有右侧刻度的部分，通过观察它，估值中的一些细节可以被很好地观察到。正中央的状态500是每个片段的第一个状态，却很少在片段中被再次访问到。平均来说1.37%的时步被用在初始状态。从初始状态单步就能到达的是第2类最常访问到的状态，可见有0.17%的时步被用在这些状态中。从这些状态开始，分布d近乎线性地降级直至最边缘处状态1和状态1000的0.0147%。这种分布最直观的效果就是图中最左侧分组，该组中的状态估值明显比无权重的平均真值要高一点，同样最右侧的分组，状态估值相应的比平均真值要低一点。该现象是由于在d分布下，这些组的状态权重有极大的不对称性。比如，在最左侧组中，状态99的权重要比状态0高出3倍。因此改组的估值就偏向于比状态0的真值要高的状态99的值。



9.4 线性（回归）法

​	在估值函数中最重要的函数之一就是估值函数xxx是权重向量xxx的线性方程组。对于任意s，存在实值特征向量xxxxxxx，具有与θ相同的分量数。特征向量可以通过不同途径从状态中构造出来。在下一节我们给出了一些可选项。无论特征如何构造，状态估值函数都是有xxx和xxx的内积给出的：

xxx （9.8）

在该式中我们称估值函数和权重具有线性相关性，简单地说就是线性的。其中的独立函数xxx被称为“基函数”，因为在该形式下它们构成了这一系列估值函数的线性基础。	构造n维特征向量来表示状态等同于选择一组有n个基函数的函数组。

​	在线性函数拟合中使用SGD更新状态值是很自然的。该例中基于xxx的估值函数的梯度是：

xxx

因此，在使用线性函数的情况下，一般SGD更新（9.7）就简化为一个特别的形式了。

​	由于其足够简单，线性SGD成了人们最喜欢用于数学分析的方法。几乎所有用于各类学习系统的收敛结果都来自于（原文for，觉得应该是from）线性（或更简单的）函数拟合方法。

​	特别地，在线性方法中只有一个最优解（或者在不理想的情况下，有一组同样好的最优解），所以这种情况下任何一种能保证收敛至近似或者局部最优的方法皆等同于收敛至全局最优。比如，如果基于一般条件，学习率xxx随时间递减的话，那么前面章节所述的梯度蒙特卡罗算法就能通过线性函数拟合来收敛至MSVE的全剧最优解。

​	前节所述的半梯度TD(0)法也能通过线性函数拟合收敛，不过该结论并不是基于一般SGD的结果；这需要用另一个定理来说明。别收敛的权重向量也不是全局最优的，而是在一个局部最优解的附近。该方法很重要，有很多细节需要深入思考，尤其是在持续性任务（case）中。基于该方法，在每个时刻t的的更新可表示为：

xxx  （9.9）

其中我们使用符号简写xxx=xxx。当该系统达到稳定的状态后，对于任意给定xxx，下一次更新的期望权重向量可写为：

xxxx （9.10）

其中 

xxxx （9.11）



从（9.10）可以看出，若该系统是收敛的，其一定收敛于权重向量xxx，其中：

xxx  （9.12）



该量（quantity）被称作TD固定点（ TD fixpoint）。事实上线性半梯度TD(0)法收敛于该点。一些理论证明了其收敛性。而下图框中给出了上式中逆矩阵的存在证明。



图xxxxxxxxx  线性TD（0）方法的收敛性证明

什么样的属性才能保证 线性TD（0）方法（9.9）的证明呢？将式（9.10）重写为如下形式可以获得些灵感：

xxx   （9.13）

注意，矩阵A是和权重向量xx而不是b相乘的；只有A对收敛有影响。为了便于理解，考虑A为对角矩阵这种特殊情况。若所有的兑奖元素为负，则其相应的的矩阵元素xxx会比1大，随之相应的xxx的元素会被放大。如果更新持续进行，最终导致结果发散。另一方面，若A的所有对角元素均为正，则xxx

可选择一个比 1/X(X为A对角元素中的最大值)更小的值，如此同其他介于0-1间的所有对角元素，xxx也成为了对角元素。 在该情况下式中第一项会趋向于压缩xxx，如此便保证了结果的稳定性。一般情况下，当A是正定的，即对所有实向量xxx有xxx时，xxx会被递减至0。正定性同样保证了逆矩阵xxx的存在。

​	对于线性TD（0），在xxx<1的持续性任务中，矩阵xxx(9.11)可写为：

xxxx

基于策略π，xxx代表平稳分布，xxx这是从xxx到xxx的转移概率，P是这些概率的xxx行xxx列矩阵，xxx是xxx行xxx列的的矩阵，其中xxx是其对角项。而xxx是xxx行xxx列的矩阵，其中xxx是其行（向量）。从这里能很清楚地看到气内矩阵xxx对A的正定性起关键作用。

​	诸如此类的关键矩阵，只要其所有列之和为非负数即可保证正定性。这是由Sutton (1988, p. 27)基于上述2条定理而提出的。定理1：对任意矩阵		M,当且仅当其对称矩阵xxxxx是正定的，则M为正定矩阵。定理2：对任意对称实矩阵S,若其对角元素值均为正，且（任意一个对角元素）值比其相应的非对角元素之和大(Varga 1962, p. 23)，则S为正定矩阵。这里我们的关键矩阵xxx，其对角元素为正，其他元素为负。所以只要证明每行之和加上对应的列之和为正即可。因为P是随机矩阵且xxx所以行之和一定为正。那么现在仅需证明列之和为正。注意，对于任意矩阵M,由其列之和构成的行向量均写成xxx,其中xxx是元素均为1的列向。记d为xxx的xxx向量。因为d是平稳分布，所以xxx=xxx。 那么现在我们的关键矩阵的 列之和可表述为：

xxxx      （因为d是平稳分布）

该关键矩阵的所有元素均为正。因此，该矩阵及其A矩阵均为正定矩阵，而在策略TD(0)法也被证明是稳定的。（若要证明其收敛概率为1，需要一些额外条件以及对随时间递减的xx的调整。）

图xxxxxxxxx

​	当θ在TD固定点，其MSVE已被证明（在持续的情况下）仅在最小可能误差的有界扩展内波动：

xxx （9.14）

也就是说，TD法的近似误差（ asymptotic error）小于等于xxx乘以最小可能误差的值。这个最小可能误差在蒙特卡罗法的极限中可获得。由于xxx往往近似于1，该膨胀因子会很大。因此TD法的金策性能会有实质性的潜在损失。另一方面，正如我们在第6,7章看到的那样，TD法比蒙特卡罗法更快。这是因为相较于蒙特卡罗法，TD法往往极大地奖励了估值的不确定性（variance）。那么到底该用哪个方法呢？这取决于问题和模拟方法的属性，以及学习所需持续的时间。

​	在策略的bootstrapping法也有类似于（9.14）的约束。例如，根据在策略分布来备份的线性半梯度DP法（xxx）也会收敛至TD固定点。单步半梯度“动作值“（action-value）类算法，诸如下章提到的半梯度Sarsa（0）法，也有个类似的约束，其收敛至一个类似的固定点。对于回合制任务，情况略有不同，但也有约束（详见 Bertsekas and Tsitsiklis, 1996）。这里我们不讲用于奖励、特征及步长参数的递减的技巧性条件（ technical conditions）。欲知详情，请参考原论文（Tsitsiklis and Van Roy, 1997）。

​	保证收敛的关键在于备份的状态是服从在策略分布的。对于其他分布，使用函数拟合的bootstrapping法实际可能会发散至无穷。关于这个的一个例子和对其的解决方案将在11章论述。

示例 9.2：在1000-状态随机步行中使用bootstrapping

状态聚合是线性函数拟合的一种特殊情况，那么结合本章的内容，我们来看下在1000-状态随机步行任务中能有哪些新发现。 图9.2的左图展示了经由状态聚合（例9.1）的半梯度TD（0）（p197）法学习而来的最终状态值函数。可见TD法估计的近似值误差要远比蒙特卡罗法（图9.1）估计的大。

​	然而，TD法有着在学习率上有着很大的潜在优势。正如第7章所述，其也将纯蒙特卡罗法泛化到多步TD法。从图9.2的右图可见，在该步行任务中，使用了状态聚合的半梯度TD法和之前在19-状态随机步行任务中的表格法在结果上有着惊人的相似。这里为了达到数量上也相似的结果，我们将状态聚合改为20分组，每组50个状态。因为20个分组在数量上近似于表格法中的19个状态。



图9.2xxx



框 用n步半梯度TD法估计xxxx

xxxxxxxxxx



特别地，使用状态聚合的状态分组的组状态转移概率（指转移到相邻的分组状态，平均每50，或最多100个状态为一组）也和使用表格法的单状态转移概率类似。为了完成类比，这里我们使用和表格版任务中同样的RMS指标来衡量所有状态在前10个回合中的表现。而不用MSVE是因为其更适合于衡量函数拟合的情况。	

​	将第7章所述的n步TD算法融入半梯度函数拟合，就自然成为了我们在示例中所用的半梯度n步TD法了。类似于（7.2），其关键方程是：

xxx （9.15）

其中由（7.1）衍生出变量n步回报：

xxx （9.16）

上面的框中给出了该算法完整代码。

-----------------------------------

9.5 线性法中的特征构造

线性法的有趣之处不仅在于其可收敛，也在于实际使用时其能高效的处理数据和计算。是否真的能如此高效关键在于任务重的状态是如何被特征表示的，而这也是本节我们需深入研究的。选择合适的特征是给强化学习系统增加先验知识的一个重要途径。直观上，选择的特征应该和任务的自然属性有一定联系，带有该属性的特征具有最好的泛化能力。若我们在评估一个几何体，举例来说，我们可以选择的特征有几何体的形状，颜色，尺寸和功能。如果我们评估的是移动机器人的状态，则我们也许需要的特征包括（机器人）位置，剩余电量，最近的声纳读数等等。

​	一般来说，我们也需要这些自然属性的组合特征。这是因为线性模式屏蔽了各个特征间的互动作用。比如某个特征i只有当另个特征j不存在时才有效（good）。举例来说，在木棒平衡任务（示例3.4）中，一个高角速度是好还是坏取决于当下的角度。如果角度也高，则高角速度意味着危险，因木棒即将下落---所以是个坏状态。然而若角度较低，则高角速度意味着木棒正在纠正自己的位置---所以是个好状态。在这种有着特征互动的情况下，应该在线性法中引入组合特征来估算状态值。在接下来的小节里我们讨论了各种普遍使用的方法。

9.5.1 多项式

对于多维连续状态空间，用于强化学习的函数拟合和内插回归问题有着诸多共同点。很多普遍用于求解内插回归的多项式也可以被用于强化学习任务。这里我们仅讨论最基本的一些多项式（形式）。

​	假设某个强化学习问题中的状态空间是有xxxxx的组成的2维实向量。为了能引入特征间的交互作用，你可能会通过将权重相乘xxx，以一种合适的方式，比如xxxxxx来表示每个状态。有护着你可能会选择诸如xxxx的特征向量，将更复杂的特征交互作用引入进来。使用这些组合特征使得函数以多元二次的形式进行拟合----尽管由那些需要学习的权重组成的拟合函数依然是线性的。

​	示范的这些特征向量是特定一组多项式基函数所呈现的结果。而这些基函数可以被定义为任意维度，并可在状态变量中引入高维复杂特征组合：

图xxxxxxxx

​	对于用d个实数变量来表示一个状态的情况，每个状态s就是一个d维实数向量xxx。每个d维多项式基函数可写为：

xxx（9.17）

其中每个xxx是集合{xxx}，N≥0的一个整数。这些函数组成了N阶多项式基，包含了xxx个不同的方程。

图xxxxxxxx

​	高阶多项式允许对更复杂的函数进行更精确的拟合。但由于N阶多项式基的函数数量随着状态空间（N＞0）成指数级增长，所以一般有必要选择一个子状态集用以函数拟合。通过观察被拟合的函数属性，借助先验知识（belief），选择一个子状态集是可行的。而另一些在多项式回归中发展而来的自动选择方法则可适用于处理具有递增、非平稳属性的强化学习任务。

测验9.1  为何式（9.17）要为d维状态空间定义xxx个不同的函数？

测验9.2给出用以产生特征向量xxx的基函数定义元素xxx和xxx。

9.5.2 傅里叶基

另一种线性函数拟合是基于时间的傅里叶级数（Fourier series）。其告诉我们任意一个周期函数均可表示成以三角函数sine或者cosine为基函数的不同级别的加权和。（若满足xxx=xxx，则说对所有xxx，则f是以T为周期的函数）傅里叶级数和更一般的傅里叶变换被广泛地用于应用科学。因为基于很多理由，若被拟合函数是已知的，则（then）其基函数的权重项可以（are 一定？）用简单的形式表示。而且只要有足够多的基函数，任何函数的估计均可被精确到预期。有意思的是，尽管在强化学习任务中，被拟合的函数是未知的，傅里叶基函数依然很容易使用且在很多任务中表现不错。Konidaris, Osentoski, 和Thomas 在2011年展示了一个简单形式的傅里叶基构造的函数可以被用于多个由多维持续性状态空间构成的强化学习问题。而该函数并不需要具有周期性。

​	首先考虑下单维度的情况。由一般福利叶级数构成的一维函数有一个周期T，通过该周期将估值函数表示为正余弦函数的线性组合，而其中每个正余弦函数的周期可被T整除？（换言之，其周期可由一个整数乘以某个基准周期1/T）。不过若被拟合的是个定义在有界区间的非周期函数，则可以将傅里叶基函数的周期T设为整个区间的长度。有意思的是，这类函数仅包含正余弦基函数的构造的线性组合中的一个周期。

​	而且，若将周期设为目标区间的2倍而仅仅关心半周期（interval笔误？）[0,T/2]中的估值。



图xxxx

图9.3



则可以只使用余弦基函数。这是可行的，因为任意偶函数均可由其表示。即，仅用余弦基函数可表示任意轴对称函数。所以只要有足够多的余弦基函数，在半周期[0,T/2]区间的任意函数均可被近似估计到满意的精度。（这里的“任意函数”并不完全正确，因为严格来说必须保证函数是 mathematically well-behaved的。但这里我们忽略不计。）另外，也可仅用正弦基函数，一个全部由“奇”函数（非原点对称）组成的线性组合（来模拟估值函数）。不过一般还是用余弦基函数会更好，因为“半偶”函数比“半奇”函数更容易模拟，鉴于后者往往在零点、原点？（origin）不连续。

​	遵循该逻辑将T=2,于是得函数的定义域为T的一半[0,1],那么由N+1个方程组成的单维N傅里叶余弦基函数就是：

xxx，

其中i=xxx。图9.3展示了单维N傅里叶余弦基函数xx，其中i=xxx；xxx是个常熟函数。与多项式基函数不同，傅里叶基函数总是有界的，不需要求幂（do not require exponentiation）。

​	在多维的情况下，傅里叶余弦级数也有该性质。

---

对于一个由d阶超矩阵（原点元素在某个角）构成的状态空间，状态均为实向量xxx，xxx。在N阶傅里叶余弦基函数中每个基函数可写为：

xxx （9.18）

其中xxx=xxx，xxx∈xxx，j=xxx 且i=xxx。该公式为每个有xxx个可能的实向量xxx定义了一个函数。其点积xxx就是将xxx内的整数分别乘到每个维度中去。正如在单维情况下那样，该整数决定了（估值）函数在该维度的频率。当然，为了适用于某个具体应用，可以通过位移或缩放来调整基函数。

---

图9.4  2维傅里叶余弦基函数xxx， i = 0,1,2,3,4,5. 来自Konidaris et al. (2011)，已获得授权。

---

​	作为一个示例，考虑d=2的情况。其中xxxx，而每个xxxx。图9.4展示了基于6个傅里叶余弦基函数的情况。其中每张图由向量xxx定义（xxx代表纵轴而xxx代表行向量，i被省去了）。c中的任意个维度为0代表函数在该维度上的值恒定。所以若xxxx，则函数在2个维度的值都是恒定的；若xxx，则函数在第二维度的值是恒定的，而其值随着第一维度的频率而波动；xxx的情况也类似。若xxx，其中没有一个维度为0（xxx）。则基函数值随着2个维度同时波动，其（值变化）反映了2个状态变量间的互动。xxx和xxx给出了每个维度额频率，而它们的比值给出了互动的方向( direction of the interaction)。 Konidaris et al. (2011)发现一个现象。那就是若在诸如学习算法（9.7）半梯度TD（0），或者半梯度Sarsa（xxx）中融入傅里叶余弦基函数，则为每个基设置一个不同的步长参数是有帮助的。如果基函数的步长参数是xxx，则他们建议将其每个基函数xxx的步长设置为xxxxx(除非每个xxxx，若如此则xxx=xxx)。经发现，强化学习任务中若使用Sarsa（xxx）搭配傅里叶余弦基函数，则其表现要优于搭配其它基函数，比如多项式基和径向基。但不出所料地，在处理中断（discontinuities）的问题上傅里叶余弦基函数有些问题。这是因为除非有非常高频的基函数，否则其很难避免在中断点附近的“波动”（ringing）。

​	正如多项式基函数拟合那样，在N阶傅里叶余弦基函数中的基函数数量将随着状态空间的维度（的增加）而呈指数级膨胀。因此，若状态空间维度过高（比如：d>5），则使用基函数集的一个子集就显得很有必要了。

---

图9.5 在1000状态随机步行任务中，傅里叶基VS多项式基。展示了基于傅里叶基和多项式基，degree分别为5,10,20的MC法的学习曲线。经过粗略地优化，2者的步长参数分别设置为xxx=0.00005和0.0001。

---

根据被估函数的属性来先验地选择是可以的。而一些自动选取基函数的方法则可适应处理迭代的、非平稳的的强化学习任务。站在该角度，傅里叶余弦基函数的一个优势在于---通过设置向量xxx来解释状态变量间的一些可疑交互作用可很方便的选择基函数；而通过限制向量xxx的取值范围，估值函数可很方便的将一些可能是噪音的高频元素剔除。

​	图9.5展示了分别使用傅里叶基和多项式基的1000状态随机任务的学习曲线。一般来说，我们不推荐在在线学习中使用多项式基。

练习 9.3 为什么式（9.81）为d维定义了xxx个不同的函数？



9.5.3 粗编码（Coarse Coding）

考虑这样一个任务，其中状态是连续且二维的。该案例中一个状态就是个2维空间的点（ a point in 2-space），即由2个实分量（component）组成的向量。我们可以用“圆”（circles）来表示状态空间中的相应特征，正如图9.6所示。如果指定的状态在一个圆中，则该圆所代表的特征值为1，并称该特征是“存在的”（present）；否则特征值为0，并称该特征是“不存在的”（absent）。这种1-0值得特征被称为二值化特征。给定一个状态，通过观察存在的二值化特征，可以确定该状态处于哪些圆中。据此可以对其位置给予粗略的编码。用这种方式表示具有重叠特征的状态（尽管特征不需要是圆或二值化）称为“粗编码”。

​	假设现在有个基于线性梯度下降的函数拟合问题，考虑圆的尺寸和密度对其的影响。每个圆有个相应的权重（xxx元素），其会被学习所影响。

---

图9.6 粗编码。那些感受野（本例中为圆）重叠的特征圆的数量会影响到状态xxx到xxx的泛化。该示例中2状态共有一个共同特征，因而2状态的泛化会稍稍受到彼此的影响。

---

当训练一个状态时，和其交织在一起的（特征）圆权重都会被影响。因此根据（9.8），所有圆中的状态估值均会被影响。一个状态点所具有的的“普遍”特征（圆）越多，其估值影响就越大，正如图9.6所示。如果圆比较小，则泛化影响会在一个较小区域内，正如图9.7a所示。而如果圆比较大，则泛化的影响会在一个较大的区域内，正如图9.7b所示。而且，特征的形状会决定泛化的属性。例如图9.7c所示，若特征的形状不是严格意义上的圆，而是在某个方向上有所拉伸的圆，则其泛化后（的形状）也会有类似特征。

​	有较大感受野的特征会给予更广泛的泛化，但这也会造成学习到的函数估计过于粗略。也就是说无法给出比感受野的宽度更精细的辨识度（discrimination）。幸运的是，情况并非如此。从一个点到另一个点的泛化一开始确实是受到感受野的大小及形状影响。但是锐度（acuity），即最终可达到最好的辨识度却可能更多地被特征的总数所影响。



示例 9.3： 粗编码的粗略度 	本示例将阐述粗编码的感受野尺寸是如何影响学习效果。该示例用基于粗编码和（9.7）的线性函数拟合法来学习一个一维方波函数（图9.8顶部所示），其中该函数值作为目标值XXX。由于只有一维，感受野是个区间而不是圆。如图底部所示，我们分别通过3个尺寸的区间（窄，中，宽）来观察学习效果。3个示例均由相同数量（density）的特征数。数量比学习的函数数量级（extent）多50个左右。训练样本均匀地从基于该量级的样本集中随机抽取。步长参数为xxx，其中m代表某一刻存在的特征数量。图9.8展示了在三个示例中函数的学习过程。注意，在学习初期特征的宽度对学习有很强的影响。较宽的特征，意味着泛化效果也会较宽；较窄的特征，则只有距离训练点较近的几个特征会被改变，这也使得学习到的函数更加的颠簸。然而，特征的宽度对最终习得的函数仅有轻微的影响。（可见）感受野的形状对泛化有很强的影响，但对近似结果的质量影响很小。

---

图9.8 特征的宽度对初期泛化（第一行）有较强影响，而对结果精确度的近似有较弱的影响（最后一行）。

---

9.5.4 瓦片编码（Tile Coding）

瓦片编码是用于多维连续空间的一种粗编码形式。其具有计算上的高效性及灵活性。对于现在的顺序数字计算机（ sequential digital computers），其可能是最实用的特征表示方法。开源软件中有很多使用瓦片编码的案例。

​	在瓦片编码中，特征的感受野被组织为输入空间的分区（元素集）。我们称每一个分区为一个“瓦块”（tiling），并称分区的每个元素为“瓦片”（tile）。例如图9.9左侧所示，作用于二维状态空间的最简单的瓦块是一个均匀网格。相较于图9.6中的圆，这里的瓦片或者说感受野是方块状的。若只使用1个瓦块，则只需用图中白点所处的瓦片来代表其状态即可；处于同一瓦片的所有状态均会被泛化，而其他状态则不会。仅仅使用1个瓦块，其实际等同于状态聚合而不是本章所说的粗编码。

​	为了利用粗编码的优势，我们需要重叠的感受野。而根据定义，各分区的瓦片是不能重叠的。为了在瓦片编码中做到真正的粗编码，我们需要用到多重瓦块。以一瓦片的部分宽度为基准，瓦块彼此间偏移一定距离。在图9.9的右侧展示了使用4瓦块的简单示例。每一个状态，比如图中的白点，每个瓦块中的一个瓦片和其交织在一起。当该状态出现时，交织的4个瓦片被激活，并成为了该状态的4个特征。特别地，特征向量xxx为每个瓦块中的瓦片配置了一个元素。在本例中总共有xxxxx=64个元素，除了4个置于s的瓦片外，其余所有瓦片值为0。图9.10展示了在1000步随机步行任务中多重偏移瓦块（粗编码）比单瓦块的优势所在。

​	由于瓦片是基于分区工作的，所以在实际任务中，关于瓦片编码的直接优势在于任一时刻，任一状态的激活特征数量都是相同的。正因为每一瓦块中只有一瓦片是激活的，所以用以表示某一状态的总特征数总是和瓦块数量保持一致。这样的特性使我们可以很容易地依直觉来设置步长参数xxx。例如，选择xxxx，其中m是瓦块的总数。这样使其转化为了 one-trial learning。在xxxx的样本训练中，无论先验估计xxx如何，新的估计将是xxxx。一般为了更好地处理目标输出的泛化和随机波动，人们希望更新的步伐能更慢一点。例如，可以选择xxxx，在该例每一次更新迭代中，每一训练样本的估计值会向目标值靠近1/10的幅度。

---

图9.9

图9.10

---

而其邻域状态（ neighboring states）的移动幅度会更小，该幅度大小与状态共享的瓦片数成比例。

​	由于使用二值化特征向量，瓦片编码也具有计算优势。因为每个元素不非0即1，几乎不会吹灰之力就可以计算出构成（9.8）函数值估计的权重求和了。想比于做n次乘法和加法，使用瓦片编码仅需计算xxx个激活的特征的指数并对权重向量中的m个相应元素进行求和即可。

​	瓦片编码的泛化不仅仅作用于被训练的那个状态，也作用于其他坐落于相同瓦片的状态（集）。被泛化的状态总数一般和瓦片数成正比。即便对瓦块间彼此的偏移也会影响到最终的泛化效果。如图9.9所示，如果瓦块彼此在不同维度偏移相同的距离，则不同的状态会有不同的泛化质量，如图9.11上半部分所示。这8张子图皆展示了一个被训练的状态是如何影响到其周边状态的泛化模式的。在该例中总过有8个瓦块，因此给定一个（状态坐落于的）瓦片，那么会被泛化到的子域（subregion）一定是64个子域中的一部分。但（无论是哪8个子域），其泛化的模式一定是图中8个中的一种。注意到依赖于平均偏移，其所构成的泛化对角上的状态集有着很强的影响。只要保证瓦块的偏移在每个维度上不平均，就可避免该人为特征。这种低层的泛化模式会（比平均偏移的模式）更好，因为所有被泛化的临近（瓦片）状态都围绕着被训练的那个状态，并没有明显的对称属性。

​	无论哪种情况，哪个维度，所有瓦块的偏移距离都是单个瓦片宽度的一部分。令xxx表示瓦片的宽度，xxx表示瓦块的数量，则xxx就是偏移距离的基本单元。在某个（方向的）小方格xxx内的所有状态会激活相同的瓦片，即具有相同的表征（ feature representation），以及相同的近似值。若状态在任意笛卡尔方向上偏移了xxx个位移，则在每个瓦片上，其表征会有1个元素（译者：即1个维度）被改变。平均偏移的瓦块彼此间正正好好偏移了一个单元距离。以二维状态空间为例，当我们说每个瓦块偏移了（1，1）个位移矢量（ displacement vector），意味着每个瓦块和其前瓦片的偏移距离等于 xxx乘该矢量。按照这种说法，图9.11下半部分的非对称瓦块的偏移矢量就是（1,3）。

​	关于不同的位移矢量对瓦片编码的影响，已有诸多学者对此作了广泛的研究。对如之前看到的偏移了（1，1）个位置的矢量对角特征， (Parks and Militzer, 1991; An, 1991; An, Miller and Parks, 1991; Miller, Glanz and Carter, 1991)对其同质性和趋势作了相应评估。根据他们的研究，  Miller and Glanz (1996)推荐使用基于第一奇整数的偏移矢量。特别地，对任意d维连续状态空间，位移矢量应该是第一奇整数xxxxxx，而k（瓦块的数量）的取值应该保证2的k次方大于等于4d。其效果正如图9.11下半部分所示，在图中d=2,xxxx,位移矢量为（1,3）。而在三维状态空间中，4个瓦块（译者：根据规则应该有4个瓦块）的偏移基坐标是xxx,xxx,xxx和xxx。给定任意d，可以用一些开源软件根据次规则来高效设置瓦块。

​	若选择瓦片编码，则有必要设置瓦块数量和瓦片的形状。瓦块的数量和瓦片大小决定了渐进拟合的精度，正如图9.8中一般的粗编码所展示的那样。而瓦片的形状则决定了泛化的属性，如图9.7。若是正方形的瓦片，则泛化效果在每个维度上是大致相等的，如图9.11（下半部分）。如果瓦片沿某方向拉升，比如图9.12b中的条状瓦片，则泛化效果会沿着该维度变化。图9.12b中的边界在左边更密集也更薄，从而沿着该维度水平方向促使瓦片在较低值有着更好的辨识度。另外，图9.12c的对角条瓦片则会促使泛化沿着某个对角而变化。在更高的维度中，轴对齐的条状瓦片会忽视掉一些瓦块的维度，即超平面切片。图9.12a中的不规则瓦块虽然也是一种选择，但很少在实际中使用；标准软件也无法驾驭这类瓦块。

​	实际上，不同的瓦块搭配不同形状的瓦片的做法更受欢迎。例如，同时使用一些水平条状瓦块和垂直条状瓦块，这一样做会鼓励在每个维度进行（相同程度的）泛化。然而若在某个特定水平和垂直条内的坐标有个特别的值，这个情况仅仅用条状瓦块是无法学习到的（因为无论学到了什么都会渗透到相应的水平或垂直条内的坐标集中）。为了做到这点，需要如图9.9所示的连接矩形瓦片。有了垂直的，水平的以及一些连接形态的瓦块，就可以做到所有的事情：对任意维度都有相应的泛化偏好，而又不失对某些连接点内特殊值的学习能力（16.3节有一个使用该方法的案例）。对瓦块的选择会影响到泛化的效果。因此在还无法有效自动化选择前，保证选择的灵活性及保证瓦块的可读性是重要的。

​	图9.12：xxxxxxxxxxxxxxxx

​	另一个对降低内存需求有效技巧是”哈希“（hashing）---通过一个不变的伪随机法将较大的瓦块分解为一个有诸多小很多的瓦片组成的集合。哈希由通过随机遍布在整个状态空间的非连续非连接区域组合而成，但这样依然依然过于庞大。例如，一个瓦片可以由4个自瓦片组成，如右侧图。通过哈希，只需消耗少量的性能即可换来内存需求的极大降低。之所以可行是因为只有状态空间的一小部分才需要足够高的精度。通过哈希可解决维数灾难。因为任务所需的内存不再是状态空间维数的指数级大小了，而只需要和实际需求大小相符即可。现在已经有很多足够好的开源软件可以操作包括哈希在内的瓦片编码了。

练习9.4	在某二维状态空间中，如果已知某个维度比另一个维度对估值函数有更大的影响，那就应该着重泛化该维度而不是沿着该维度泛化。那么如何设计瓦块才能利用好该先验知识呢？

9.5.5 径向基函数

​	径向基函数（RBFs）是粗编码的一种形式。是为了应对连续值特征的一种自然演化。在该形式下，其特征取值不再仅限于0或者1，而是可以在区间xxx间的任意值。这些取值反映了各个不同特征的表现“程度”（degree）。一个典型的RBF特征,i具有一个高斯(钟形)响应函数xxx。其取值仅受状态s和特征原型（中心状态）xxx的距离以及特征宽度xxx的影响：

xxxx

根据当前的状态或任务来选择看上去最合适的范数或距离标尺是可以的。图9.13展示了一个使用了欧几里德标尺的一维特征例子。

​	相较于二值特征，RBFs的一个主要优势在于其拟合函数表现得非常光滑，和二值特征的函数有显著的区别。 这很吸引人，但在很多案例中却并没有实际意义。尽管如此，诸如在瓦片编码中应用RBFs等分级响应函数（ graded response functions）的情况也已被广泛地研究了（xxxx）。所有这些方法无一例外地需要（比瓦片编码）更多的计算复杂度。也因此，在状态空间大于二维时，往往会降低性能。在高维空间，瓦片的边缘（the edges of tiles ）会变得更加重要。另外，已有相关证明表示在边缘附近有效地控制分级瓦片激活器（graded tile activations）并不容易。

​	RBF“网络”（network）是个线性函数拟合器，其使用RBFs作为其特征。学习过程和其他线性函数拟合器一模一样，正如公式（9.7）和（9.8）所展示的那样。另外，一些作用于RBF网络的方法啊改变了特征的中心值和宽度，将其带入了非线性函数拟合的领域。非线性方法可更精确地拟合目标函数。但其却缺点在于当其作用于RBF网络，尤其是非线性RBF网络时，其计算复杂度会变得很高。并且在学习能保证稳定高效之前，往往需要大量的手动调参。



9.6 非线性函数拟合：人工神经网络

 人工神经网络（ANNs）被广泛用于非线性函数拟合中。ANN是一个有着多个内连单元的网络，这个单元有着类似于神经元的属性，而神经元是构成神经系统的主要组成部分。  ANN的发展有着很长的历史。最新发现，在包括强化学习等在内的机器学习领域，由其构成的深层ANN均有着不错的表现。在第16章我们介绍了些令人振奋的的强化学习示例，它们也使用ANN进行函数拟合。

​	图9.14展示了一个通用前馈ANN,前馈意味着在该网络中是没有环的，即没有一个单元的输出是可以影响到该网络的输入值的。在该网络中有一个由2个输出单元组成的输出层，一个由4个输入单元组成的输入层以及2个隐含层：既不是输出也不是输出的层。 网络中实值权重与每个连接相关联。该权重类似于人脑（real）神经网络（见15.1节）中的突触效能（ efficacy of a synaptic）。如果ANN的连接中有至少一个环，则其是一个反馈而不是前馈ANN。尽管强化学习中2种网络均有应用，但这里我们主要研究更简单的前馈ANN。

​	这些单元（图9.14中的环）是典型的半线性单元。这意味着通过这些单元将计算带有权重的输入信号并将结果输出至一个非线性函数，也称为“激活函数“（ activation function）。尽管有很多不同的激活函数可供使用，但基本都是S（sigmoid）形的函数，比如逻辑函数xxx。有的时候诸如xxx的非线性整流器（ rectifier nonlinearity）也会用到。另外若使用阶跃函数，比如xxx，当xxx，否则为0。则结果会随着界限值xxx变成一个二值单元。不同的层使用不同的激活函数往往是有用的。

图xxxx

图9.14 一个通用的前馈神经网络。其有4个输入单元，2个输出单元，以及2层隐含层。

图xxxx

​	在一个前馈ANN中，每个输出单元的激活值就是在网络中对输入单元求其激活模式的非线性函数。这些函数以网络中的连接权重为参数。一个没有隐含层的ANN只可能输入-输出函数做出有限的调整。然而带有隐含层的ANN却可以在输入空间的某个紧凑区域内，利用大量有限但足够多的sigmoid单元来拟合任意的连续函数（Cybenko, 1989）。只要满足温和条件（ mild conditions），该论点也适用于非线性激活函数。但必须保证非线性的激活函数：因为若一个多层前馈ANN中所有单元使用的是线性激活函数，则整个网络等价于一个没有隐含层的网络（因为线性函数的线性函数是自自线性的）。

​	理论和实践均已证明，单层ANN除了其所具有的”广泛适用的拟合“属性外，从前诸多人工智能任务中所需的复杂函数的拟合工作也比以前容易多了----为了达到目标，确实可能需要---诸多的抽象。这种抽象由诸多低层次的抽象分级组成。而低层次的抽象又由有着许多隐含层的深度模型抽象而成。(详见Bengio, 2009) 深度ANN的后继层又计算出了对于网络的”原始“输入更抽象的表述。其中每个单元代表了一个特征，作为整个网络的输入-输出函数中的的分级表述。

​	在人工智能领域，如何在不依赖于大量的手工特征的情况下，创造出上述的分级表述是个持久的挑战。这也说明了为何使用含隐含层的ANN学习算法在近些年获得了大量关注。一个典型的ANN是通过随机梯度法来学习的（图9.3）。每个权重通过往某个方向调整一点点来提升整个网络的性能。该性能由一个要求结果最大化或者最小化的目标函数来衡量。在普遍的监督学习中，给定一系列标签训练样本后，所谓的目标函数是期望误差或损失。而在强化学习中，可以将TD误差用于ANN中来学习估值函数，或者用于梯度bandit（2.7节）、策略梯度中来最大化期望回报。在所有这些案例中，如何评估在那些交织在一起的权重中，单个权重的改变对整个网络性能的影响是很有必要的。换言之，在给定当前网络的所有权重值的情况下，针对每个权重，即对于目标函数的偏导，评估其对整个网络性能的影响。所谓的梯度就是由这些偏导组成的向量。

​	作用于含隐含层的ANN的最成功的学习方法是反向传播算法，其通过贯穿于整个网络的前馈和反馈过程来执行。在前馈过程中，给定当前网络的输入单元的激活值后，会计算一遍所有单元的激活值。在每一轮前馈过程后，反馈中则高效地计算了每个权重的偏导。（就像在另一个随机梯度算法中一样，这些偏导组成的向量是对真实梯度的估计值。）在第15.10节我们讨论了一个利用强化学习原则而不是反向传播的方法来训练含隐含层的ANN。相比于反向转播，这些方法有些低效。但其更接近于真实的神经网络工作机制。

​	在仅有1、2层的浅层的神经网络中使用反向传播算法可以取得不错的结果，但在更深层次ANN中却不怎么好。事实上，一个有着k+1 隐含层的ANN训练结果是可能会比含k个隐含层的ANN的训练的训练结果差的。即便深层的ANN能表示所有浅层ANN的函数，其结果也同样如此 (Bengio, 2009)。哟解释这个现象可不容易，但确有几个关键因素。首先，一个有着大量权重的典型ANN将很难规避过拟合的问题。即，无法对未经训练的样本给出足够正确的估计。其二，反向传播之所以不适用于深度ANN是因为经反向传播计算而来的偏导会朝着网络输入方向快速衰退而导致学习过慢，或者是快速上升而导致学习过于不稳定。如何处理这些问题将极大地影响深度ANN所带来的很多令人瞩目的结果。

​	在有限的训练数据中通过消耗自由度来调整函数时，会遇到过拟合问题。虽然在在线强化学习这种样本数量并不受限的任务上不是很成问题，但要做到有效的泛化依然是个不可忽视的要点。在ANN中普遍存在过拟合的问题。这个问题在深度ANN中尤其严重因为其中有大量的权重需要调整。现在已有很多方法被研究出来用以对抗过拟合问题。比如可以在验证数据集变现开始劣于训练集时停止训练（交叉验证）；也可以通过调整目标函数来惩罚过于复杂的拟合行为（正则化）；另外也可以引入权重间的依赖性来江都自由度（比如权重共享）。

​	一个可以显著降低过拟合影响的方法是由 Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov (2014) 提出的剔除法（dropout method）。在训练中，有些单元连同其连接被随机地剔除出网络。可以视其为在训练一个”更瘦“的网络。在测试的时候将经由更瘦的网络所训练出的结果混合进来将有助于提高泛化的性能。剔除法通过将每个输出的单元权重乘以该单元在训练中被保留下来的概率来高效地模拟整合过程。Srivastava et al.发现该方法显著地改进了（网络）泛化的性能。它允许单个隐藏单元学习出可以和其他随机选择的特征良好协作的特征。如此就增加了隐藏单元特征的多样性，从而使得网络并不会因为一些极少出现的情况而被训练得过于特殊化。

​	在解决深度ANN中的过拟合问题上Hinton, Osindero, and Teh (2006)取得了重大进步。它们所使用的用以训练的深度置信网络结构和这里讨论的深度ANN很类似。在他们的方法中，最深层的权重通过使用非监督学习算法训练，每次只训练一个参数？。不依赖于总体目标函数，无监督学习可以提取捕获输入流的统计规律的特征。首先最深层最先被训练，然后依赖于该层的输出，倒数第二深的层被训练，以此类推。这个过程一直持续到所有或者大部分层的权重值被调整至类似于可被用于监督学习的初始化值状态。然后该网络以目标函数为基准利用反向传播算法对值进行微调。相关研究表明这种方法一般比使用随机初试值的反向传播算法更加有效。要想通过使用该方法初始化权重来提高网络性能需要注意诸多因素。但一种思想认为（不管怎么说），该方法使得将网络置于一个权重空间，使得基于梯度的算法能表现得更好。

​	有一类叫做“深度卷积网络”（deep convolutional network）的深度ANN被证明在实际问题中有着非常成功的应用，包括瞩目的强化学习问题（第16章）。这类网络被专门用于处理一些诸如图片这类在空间数组中排列的高维数据。该网络的设计受到了大脑中早期视觉形成工作的启发 (LeCun, Bottou, Bengio and Haffner, 1998)。由于其特殊的构造，可直接使用反向传播来训练。而不用考虑之前描述的用于深度层的训练方法。

​	图9.15展示了深度卷积网络的架构。该实例由 LeCun et al. (1998)设计用以识别手写字符。其中包含调整过的卷积层和抽样层，在这些层后是一些全连接终层。每一个卷积层产生一系列特征映射。特征映射是个由单元数组组成的活动模式（a pattern of activity）。而每个单元在其感受野里对数据做着相同的操作，即通过上层（对于本例的第一层的卷积层，也可以是外部输入数据）来“看到”数据。特征映射的单元均是彼此独立的，但其感受野全都是一样的，不论是尺寸还是形状。其坐落于不同输入数据数组中的不同位置。在同一特征映射的单元共享权重。



图xxxx

图xxxx

这意味着特征映射会检测到同一个特征而无论其在输入数组中的何处。以图9.15中的网络为例，其第一个卷积层产生了6个特征映射，而每个由28*28个单元组成。而每个特征映射中的单元又有个xxx的感受野。这些感受野相互重叠（该例中是个4列5行的块）。结果就是，该6个特征映射中的每一个只有25个可供调整的权重。

​	深度卷积网络中的抽样层降低了特征映射中的空间分辨率。抽样层中的每个特征映射由诸多单元组成。而这些单元数值由上层卷积层的特征映射单元的感受野平均而来。以图9.15的网络为例，其第一个抽样层的6个特征映射的每个单元均是由第一个卷积层的某个特征的2x2非叠加感受野平均而来。计算结果为6个14x14的特征映射。该抽样层将网络的灵敏度降低至探测到的特征的局部空间。即，抽样层帮助网络对空间中的不变部分做出合适的反应。这样做是有意义的，因为一个在（图片）某区域被探测到的特征是可能在其他区域被复用的。

​	关于ANN的设计及训练优势，这里我们仅提到了部分适用于强化学习任务的内容。尽管当前强化学习理论主要局限于表格或者线性函数拟合法，但正因为ANN,尤其是深度ANN的非线性函数拟合法的加入，才有了一些瞩目的强化学习任务中的杰出表现。

​	

9.7 最小二乘TD(Least-Squares TD)

​	在9.4节，我们通过线性函数拟合建立了TD(0)方法，其能保证渐进收敛。只要做到适当地减少补偿，直至TD固定点：

xxx，

其中有

xxxxxxxxxx

我们也许会问，为何一定要迭代地计算该解决方案？这是在浪费数据！为何不通过计算xxx和xxx的估计值，再直接计算TD固定点这样来更有效率的利用数据呢?最小二乘TD(通常称为Least-Squares TD)就是基于这样的思想而诞生的。其估计式样是：

xxx （9.19）

（其中xxx，对于一些较小的xxx>0，能保证xxx是可逆的）然后估计TD固定点：

xxx（9.20）

这是线性TD（0）中最高效使用数据的一种算法。但其也有较昂贵的计算开销。记得半梯度TD(0)算法的内存和单步计算开销仅有xxx。

​	那么LSTD有多复杂？根据上述公式，其复杂度似乎随着xxx的增加而增加。根据（9.19）的2个近似，使用之前所述的技巧（比如第2章）可以递增地计算LSTD，所以每一步（更新中）只需要固定时常。虽然如此，xxx的更新离不开外积（列向量乘以行向量），所以这是一个矩阵更新；其所需的I计算复杂度是xxx，自然为了存储xxx矩阵其所需的内存有xxx。

​	一个更大的潜在问题是我们的最终计算步骤（9.20）需要用到xxx的逆，而一般求逆的计算复杂度是xxx。幸运的是，以外积之和这类特殊形式组成的矩阵，求逆仅需要xxx的复杂度来完成递增更新：

​	xxxxxxxx

其中xxx=xxx。

虽然被称为”谢尔曼 - 莫里森公式“（ Sherman-Morrison
formula）的式（9.21）表面上看起来很复杂，但其实际上仅包含向量-矩阵和向量-向量的乘法，算法复杂度实际为xxx。因此我们可以保存矩阵xxx的逆，然后在式（9.20）中使用它。完成这些只需要xxx的内存和单步计算。在下页（译者：原著中也不是下页，而是上页）中给出了完成的算法。



图xxx

图xxx

​	当然，xxx的开销依然远大于半梯度TD法中的xxx开销。为了达到LSTD中的高效数据利用率，是否值得承受如此大的计算开销呢？这取决于xxx的大小，学习速度的重要性以及系统其他部分的开销。关于LSTD，其不需要步长参数优势是有提及，但该特点的优势却可能被过度夸大了。以为其虽不需要步长参数，却仍然需要xxx；如果选取的xxx很小，则逆序列（ the sequence of inverses）的波动幅度会很大，反之若选择太大，则学习速率会太慢。另外，因为LSTD没有步长参数，所以其永不忘记（数据）。有时候这是我们所需要的，但当在强化学习和GPI任务中需要策略xxx改变时，该特点会变得有些麻烦。在控制应用中，典型的做法是将LSTD和其他机制结合在一起来诱使其健忘，进而消除因不需要参数而来的任何初始优势。



9.8 总结

​	为了适用于人工智能或大型工程应用，强化学习系统必须有足够的“泛化”能力。为了做到这点，在“监督学习”中任何已存在的“函数拟合方法”均可被使用。只要简单地将备份（backup）用作训练样本即可。

​	也许最适用于强化学习的监督学习方法是“含参数的函数拟合法”。其中策略被参数化为权重向量xxx。尽管权重向量中已有足够多的元素，其表示的状态空间依然很大，我们必须寻找一个拟合的解决方案。针对权重向量xxx，在“在策略”分布d的条件下，我们定义xxx作为衡量xxx的误差函数。在在策略的案例中，xxx清晰地给了我们一个衡量不同函数拟合的标准。

​	为了找到好的权重向量，最流行的方法是“随机梯度下降法”(SGD)的变种。本章我们聚焦于使用了“固定策略”（fixed policy）的“在策略”，也称为策略更新或预测；适用于其的一个自然（学习算法）选择是“n步半梯度TD法”（ n-step semi-gradient TD），其中当n=xxx或n=1时，引出了2种特殊算法，梯度MC和半梯度TD(0).半梯度TD法不是真正的梯度法。因为在这类bootstrapping法（包括DP）中，虽然权重向量有在更新目标中出现，但在计算梯度时却没有被考虑---因此这类算法被称为“半”梯度算法。所以这类算法的结果不能参考经典的SGD。

​	然而，使用了“线性”函数拟合的半梯度法的结果还是不错的。在线性函数拟合中，被估值等于权重和特征间乘积的和。在理论上，基于线性的案例是最好理解的。其实际表现也不错，只要有合适的特征。为了将先验知识加入强化学习系统，特征选取是最重要的途径之一。这些特征可以是多项式形式的，过去其在在线学习的模式下泛化得并不好，尤其是强化学习中。更好的是根据傅立叶基选择特征，或根据具有稀疏重叠感受野的某种形式的粗编码。瓦片编码是粗编码中的一种，其计算效率尤其高而且灵活。径向基在一二维的任务中很有用，这种任务中其平滑变化的响应是重要的。LSTD是最高效的数据线性TD预测方法，但其所需的计算资源正比于权重数量的平方。然而其他方法的复杂度（仅仅）正比于权重数量。在非线性方法中，有基于反向转播和SGD变种来训练的人工神经网络；近年来名为“深度强化学习”（deep reinforcement learning）的这些方法变得非常热门。

​	对所有n并基于最佳误差界限内的MSVE，线性半梯度n步TD法保证在标准条件下收敛。 这个界限是对较高的n总是更紧密，并且当n→∞时接近零。然而，实际情况下学习会非常慢，所以一定程度（1 < n < ∞）的bootstrapping往往更受青睐。

​	

参考书目和历史备注

​	泛化和函数拟合一直是作为一个整体出现在强化学习领域的。Bertsekas and Tsitsiklis (1996), Bertsekas (2012), and Sugiyama et al. (2013)提出了应用于强化学习的一些关于函数拟合的先进方法。在本节最后会讨论强化学习中关于函数拟合的一些早期工作。



9.3	 在监督学习中用于最小化均方误差的梯度下降法是众所周知的。 Widrow and Hoff (1960)引入了最小均方算法（LMS），作为递增梯度下降法的原型。有许多文献可参考到与此相关的内容（比如：Widrow and Stearns, 1985; Bishop, 1995; Duda and Hart,1973）。

​	半梯度TD(0)法第一次被Sutton (1984, 1988)所发现。其也是线性TD(XXX)算法中的一员。关于TD(XXX)将在12章讨论。用“半梯度”来描述这些bootstrapping法是本书第二版新引入的。

​	最早在强化学习中使用状态整合的可能是Michie and Chambers’s BOXES system (1968)。关于在强化学习中使用状态整合的理论研究已由 Singh, Jaakkola, and Jordan (1995) and Tsitsiklis and Van Roy (1996)所展开。在状态聚合被刚开发的时候，其已被用于动态规划 (比如： Bellman, 1957a)。



9.4	Sutton (1988) 证明了在TD(0)中使用平均、最小MSVE来衡量的话，是可以保证收敛概率为1的。其中特征向量xxx是线性独立的。同一时间其他学者 (Peng, 1993; Dayan and Sejnowski, 1994;Tsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994). 也证明了该收敛结论。另外，Jaakkola,Jordan和Singh (1994)也证明了在在线更新的情况下该结论同样成立。所有这些结果是基于独立的线性特征向量，其暗示了有多少状态就至少有多少权重元素xxx。关于更重要的案例，即包含一般（不独立的）特征向量的收敛问题研究由 Dayan (1992)首次提出。关于Dayan的研究成果的一个重要拓展和强化是由 Tsitsiklis and Van Roy (1997)证明的。他们证明了本节所述的主要结论，即线性bootstrapping法中渐近误差的界限。



9.5	我们所介绍的关于线性函数拟合的可能性范围是基于Barto (1990)相关内容的研究。



9.5.3 术语“粗编码”（coarse coding）源自于 Hinton (1984)，而图9.6是他的其中一幅作图。在强化学习系统中使用该类型的函数拟合的一个早期例子是由Waltz and Fu (1965)提供的。



9.5.4  Albus (1971, 1981)提出了瓦片编码，包括哈希（hashing）。他称其为“基于小脑模型的咬合控制器”（cerebellar model articulator controller），或者在有些文献中也被称为CMAC。虽然 Watkins (1989)已经使用“瓦片编码”来表述CMAC了，但该术语也是第一次出现在本书的第一版中。瓦片编码已被使用于诸多强化学习系统（比如：Shewchuk and Dean, 1990;Lin和Kim, 1991; Miller, Scalera和Kim, 1994; Sofge和White, 1992;Tham, 1994; Sutton, 1996; Watkins, 1989）和其他类型的学习控制系统（比如： Kraft和Campagna, 1990; Kraft, Miller和Dietz,1992）。本节介绍的内容主要来自Miller and Glanz (1996)的研究。



9.5.5 自从 Broomhead and Lowe(1988)将径向基函数（RBFs）和神经网络结合来做函数拟合后，该方法就受到了广泛关注。 Powell (1987) 评估了早期RBFs的使用，而Poggio and Girosi(1989, 1990)则广泛发展并应用了这种方法。



9.6	自McCulloch and Pitts (1943)提出将阈值逻辑引入作为抽象模型神经元的单元后，就标志着人工神经网络（ANNs）的开始了。作为分类和回归的学习方法，ANNs已经历了几个不同的阶段：大致来说，首先是单层ANNs的感知阶段 (Rosenblatt, 1962)和ADALINE (ADAptive LINear Element (Widrow and Hoff,1960) ；使用了多层ANNs来学习的误差-反向传播阶段(Werbos, 1974; LeCun, 1985; Parker, 1985; Rumelhart, Hinton,Williams,
1986)；以及当前着重于学习展示的深度学习阶段 (比如：Bengio, Courville和Vincent, 2012; Goodfellow, Bengio和Courville, 2016)。关于神经网络的很多书均来自于Haykin (1994), Bishop (1995)和Ripley (2007)。

​	将ANNs用作强化学习中的函数拟合，这种做法可以追溯到Farley and Clark (1954)建立的神经网络。他将用来表示策略的线性阈值函数的权重交给类强化学习（ reinforcement-like learning）。 Widrow, Gupta, and Maitra (1973) 展示了一个类神经元的线性阈值单元来进行学习。他们称其为“通过使用评价器或可选bootstrap适应来学习”（ learning with a
critic or selective bootstrap adaptation），即ADALINE算法的强化学习版本。 Werbos (1974, 1987, 1994)开发了一种预测控制方法，其通过ANNs的误差反向传播来学习策略，并结合类TD算法学习估值函数。 Barto, Sutton,
和Brouwer (1981) 、Barto和Sutton (1981b)将联合内存网络( associative memory network)(比如:Kohonen, 1977; Anderson, Silverstein, Ritz,
和Jones, 1977)扩展至强化学习任务中。 Barto, Anderson和Sutton
(1982)使用一个两层ANN来学习非线性控制策略，并强调了第一层用于学习一个合适的表征（representation）的作用。关于使用多层ANNs来学习估值函数，Hampson(1983, 1989)是其早期的支持者。Barto, Sutton和Anderson (1983)提出了基于ANN学习 木条平衡任务（详见15.7和15.8）的 执行-评价算法（ actor-critic algorithm）。Barto和Anandan (1985) 提出了基于Widrow, Gupta, and Maitra’s (1973) 的可选bootstrap算法的一个随即版本，称之为“联合奖励-惩罚算法”（associative reward-penalty (A R−P ) algorithm）。 Barto (1985, 1986)、Barto和Jordan (1987) 设计了一个经由全局扩散强化信号训练而来的xxx单元组成的ANNs，将其用于学习非线性可分的分类规则。那个时期Barto (1985)研究了如何该方法运用于ANNs并在文献中探讨了该学习规则和其他学习规则间的关系。(详见15.10节关于使用该方法来训练多层ANNs的内容。) Anderson (1986, 1987, 1989) 评估了许多用于训练多层ANNs的方法，并证明了在平衡木和汉诺塔任务中，若使用2层ANNs，借助于执行-评价算法（执行器和评价器同时作用于ANNs），并通过误差反向传播训练出来的结果要远远好于单层ANNs。Williams (1988)设计了一些方法将反向传播和强化学习同时整合进ANNs的训练。Gullapalli (1990) and Williams (1992) 设计了专用于有着连续而非二值输出的类神经元强化学习算法。 Barto, Sutton, and Watkins (1990)认为在序列决策问题中，ANNs可作为一个重要组成部分来完成函数拟合。 Williams (1992) 将“加强”（REINFORCE）学习规则（详见13.3节）加入到误差反向传播法中来训练多层ANNs。 Schmidhuber (2015) 审查了用于强化学习中的ANNs，其中包括循环ANNs。



9.7	LSTD最先由 Bradtke and Barto (参考：Bradtke, 1993, 1994; Bradtke and Barto, 1996; Bradtke, Ydstie, and Barto, 1994)提出，并由 Boyan (2002)进行了深入地研究。至少在1949年，逆矩阵的增量更新已被人们所熟知 (Sherman and Morrison, 1949)。	



​	关于使用函数拟合来学习估值函数，（我们已知）最早使用该技术的案例是“塞缪尔的跳棋选手”（ Samuel’s checkers player）(1959, 1967)。塞缪尔听取了 Shannon (1950)的建议，认为在游戏中未必需要将估值函数设计导向，以供选择有用的操作（moves），而且估值函数可以借助特征来进行线性函数拟合。在该游戏试验中，塞缪尔不仅尝试了线性函数拟合，也尝试了查找表（ lookup tables）和称为签名表的分层查找表（ hierarchical lookup tables）（Griffith, 1966, 1974; Page, 1977; Biermann, Fairfield, and Beres, 1982）。

​	在几乎塞缪尔开展相关工作的同一时间，Bellman和Dreyfus (1959) 提出了将函数拟合用于DP。（我们不禁好奇是否 Bellman和塞缪尔在彼此工作中有着相互促进作用，但并没有相关证据。）现在，关于函数拟合以及DP已经有相当数量的参考文献了，比如多网格法和使用样条和正交多项式的方法 (e.g., Bellman and Dreyfus, 1959; Bellman,Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)。

​	Holland(1986)的分类系统运用了可选特征匹配技术，来评估所有状态-动作对。每个分类器用以匹配一个特定的状态子集，该子集的部分特征有特定的值，而其他的特征值则取任意值（犹如扑克中的百搭牌“wild cards”）。然后在典型的状态聚合法中使用这些子集来进行函数拟合。Holland的想法是将遗传算法运用于一组分类器，总体来说可以被当作一个有用的动作值函数。Holland的想法影响到了早期研究强化学习的作者，但我们这里聚焦于其他不同的函数拟合方法。因为将分类器用于函数拟合是有诸多局限的。首先，其属于状态整合法，在缩放（scaling）和有效展示光滑的函数时其有着（抹之不去的）影响。另外，分类器的匹配规则仅和聚合边界有关，而该边界和特征轴是没有关系的。传统的分类器系统最主要的局限可能在于其分类器是从遗传算法（一种调优法）学习而来的。正如第1章所述，通过学习而不是调优法，agent能发现更多关于如何学习的细节信息。这个观点指引我们转而使用监督学习，尤其是梯度下降和神经网络来完成强化学习任务。Holland和我们采用的方法有着如此般的不同并不是件出人意料的事。因为Holland提出其方法的时期正好是神经网络被普遍认为因计算能力太差而表现太弱的时候。而我们开展相关工作时，正好是人们刚开始对该传统方法提出广泛质疑的时期。（应该说）依然有很多的机会将这2个不同的方法整合到一起。

​	Christensen和Korf（1986）将回归法用于国际象棋中以对其线性估值函数的系数进行修正。 之后Chapman和 Kaelbling (1991) 和Tan (1991) 采用了决策树来对估值函数进行学习。目前基于解释的学习方法也已适应学习价值函数，其能够产出足够紧凑的表征(Yee, Saxena, Utgoff和Barto, 1990; Dietterich and Flann, 1995)。

​	



​	



