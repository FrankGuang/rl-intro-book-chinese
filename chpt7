第7章多步自举

	在本章中，我们统一了前两章中提出的方法。也没有
蒙特卡罗方法和上一章中提出的一步TD方法
总是最好的。多步TD方法概括了这两种方法
可以平滑地从一个切换到另一个。它们与蒙特之间有一个频谱
Carlo方法在一端和一步TD方法在另一端，并且经常
中间方法将比任一极端方法表现更好。
  另一种查看多步骤方法的好处是，他们释放你
从时间步骤的暴政。使用一步法同样的步骤确定
可以更改操作的频率以及引导时间间隔
已经完成了。在许多应用中，人们希望能够非常快地更新动作
考虑任何已经改变的东西，但是bootstrapping如果结束了最好
其中发生显着且可识别的状态改变的时间长度。
使用一步法，这些时间间隔是相同的，因此是一个妥协
必须作出。多步方法使启动能够在更长的时间内发生
间隔，使我们脱离单一时间步骤的暴政。
  多步法通常与资格的算法思想相关联
痕迹，但在这里我们将考虑自己的多步理念，推迟
治疗资格跟踪机制直到后来，在第12章。
  像往常一样，我们首先考虑预测问题，然后控制问题。
也就是说，我们首先考虑多步法如何帮助预测回报
固定策略的状态函数（即，在估计vπ中）。 然后我们扩展的想法
行动价值和控制方法。

7.1 n阶梯TD预测
  位于蒙特卡罗和TD方法之间的方法的空间是什么？ 考虑
使用π产生的采样事件估计vπ。 蒙特卡罗方法执行
基于从其中观察到的奖励的整个序列的每个状态的备份
状态直到剧集结束。一步法的备份方法，另一方面
手，是基于只是一个下一个奖励，从一阶段后的状态值作为剩余奖励的代理引导。一种中间体
方法，然后，将执行基于中间数量的奖励的备份：
多于一个，但少于全部，直到终止。例如，两步
备份将基于前两个奖励和状态的估计值
两步后。同样，我们可以有三步备份，四步备份和
等等。图7.1用一步表示v n的n步备份的频谱
TD备份在左侧和上至终止蒙特卡罗备份上对。
  使用n步备份的方法仍然是TD方法，因为它们仍然存在
基于其与稍后的估计的不同来改变较早的估计。现在
后来估计不是一步之后，而是n步之后。方法中的时间
n个步长的差被称为n步TD方法。 TD方法
在上一章介绍的所有使用一步备份，这就是为什么我们调用
他们的一步TD方法。
  更正式地，考虑作为状态转换的结果应用于状态S t的备份，
回报序列，S t，R t + 1，S t + 1，R t + 2，...，R T，S T
性）。我们知道在蒙特卡罗备份中，vπ（S t）的估计在
方向完全返回：
[math]
  其中T是剧集的最后一个时间步长。 让我们称这个数量为目标
备份。 而在蒙特卡罗备份中，目标是一步一步的返回
备份目标是第一个奖励加上下一个状态的折扣估计值，我们称之为一步返回：
[math]
  其中V t：S→R这里是在时间t的vπ的估计，在这种情况下它是有意义的
γVt（S t + 1）代替剩余项γRt + 2 +γ2 R t + 3 + ... +
γT-t-1 R T，正如我们在上一章中讨论的。 我们现在的观点是这个想法
在两个步骤之后，就像在一个步骤之后一样有意义。 目标为a
两步目标是两步回报：
  其中现在γ2 V t + 1（S t + 2）校正不存在项γ2 R t + 3 +γ3 R t + 4 +
... +γT-t-1 R T。 类似地，用于任意n步备份的目标是n步骤
返回：
 （7.1）
  所有n步返回可以被认为是完全返回的近似值，被截断
在n个步骤之后，然后通过V t + n-1（S t + n）对剩余的缺失项进行校正。
如果t + n≥T（如果n步返回延伸到或超过终止），则所有
缺失项被取为零，n步返回被定义为等于
普通全回报
  
  注意，n> 1的n步返回涉及未来的奖励和价值函数
在从t到t + 1的转变时不可用。 没有真正的算法可以使用
n步返回，直到它已经看到R t + n和计算V t + n-1。 第一次
这些可用的是t + n。 使用n步返回的自然算法
因此
  
  而所有其他状态的值保持不变，V t + n（s）= V t + n-1（s），则S 6 = S t。
我们把这个算法称为n步TD。 请注意，在此期间不进行任何更改
每集的前n-1步。 为了弥补，相等数量的添加
更新在剧集结束时，终止后和开始前进行
下一集。 完整的伪代码在下一页的框中给出。
n步返回使用值函数V t + n-1来校正缺失的奖励
超过R t + n。 n步返回的一个重要属性是他们的期望是
在最坏状态意义上，保证是比V t + n-1更好的vπ的估计。 那
是，预期n步返回的最差误差保证小于或
等于γn + V t + n-1下的最差误差：
  
  对于所有n≥1。这被称为n步返回的误差降低属性。因为
的误差减少性质，可以正式显示所有n步TD方法在适当的技术条件下收敛到正确的预测。然后-
步进TD方法因此形成一系列声音方法，具有一步TD方法
和蒙特卡罗方法作为极端成员。
例7.1：n步TD随机游走的方法考虑使用
n步TD方法对随机游走任务描述在示例6.2中并示出
在图6.2中。假设第一次发作直接从中心状态进展，
C，向右，通过D和E，然后在右边终止与返回
回想所有状态的估计值从中间开始
值，V（s）= 0.5。作为这一经验的结果，一步法将改变
仅对最后状态的估计V（E），其将朝向1递增
观察回报。另一方面，两步法将增加这些值
：V（D）和V（E）都将递增
对于n> 2的三步法或任何n步法将增加
所有三个被访问状态的值朝向1，都是相同的量。
n的哪个值更好？图7.2显示了简单的经验测试的结果
对于更大的随机游走过程，具有19个状态（并且具有-1结果
左，所有值初始化为0），我们在本章中用作一个运行示例。
显示了n步TD方法的结果，其中n和α的值的范围。的
每个参数设置的性能测量，如垂直轴上所示
在结束时预测之间的平均平方误差的平方根
19个州的情节及其真实值，然后在前10个情节中取平均值
和100次重复的整个实验（相同的步行被用于所有参数设置）。注意，中间值为n的方法效果最好。
这说明了如何将TD和蒙特卡罗方法推广到n步
方法可能潜在地比两种极端方法中的任一种更好。

  练习7.1你为什么认为一个更大的随机游走任务（19个州而不是
5）在本章的例子中使用？一个更小的步行已经改变了
优势到不同的n值？左侧结果的变化如何
0到-1在更大的步行吗？你认为这在最好的方面有什么区别
n的值？

7.2 n步骤萨尔萨
如何使用n步法不仅用于预测，而且用于控制？在这里
我们将展示如何将n-step方法与Sarsa直接结合
方式来产生政策上的TD控制方法。 Sarsa的n步版本我们
调用n步骤Sarsa（λ），和前一章中我们提出的原始版本
此后称为一步Sarsa或Sarsa（0）。
主要想法是简单地切换状态的动作（状态 - 动作对），然后
使用ε贪婪策略。 n步骤Sarsa的备份图，如图7.3所示
类似于n步TD（图7.1），交替状态和动作的串，
除了萨尔萨的所有开始和结束与一个行动，而不是一个状态。我们重新定义
估计动作值的n步返回：
    
图7.3：状态动作值的n步备份的频谱。 他们的范围从
Sarsa（0）的一步备份到蒙特卡罗方法的直到终止备份。 在
之间是n步备份，基于n个步骤的实际奖励和估计的价值
第n个下一个状态 - 动作对，都适当折扣。 最右边是备份
n步骤图预期萨尔萨。

而所有其他状态的值保持不变，Q t + n（s，a）= Q t + n-1（s，a）
使得s 6 = S t或a 6 = A t。 这是我们称为n步Sarsa的算法。 伪码
显示在下一页的框中，以及为什么它可以加速学习的示例
与一步法相比，在图7.4中给出。
预期的萨尔萨怎么样？ n步版本的Ex-
图7.3中最右侧显示了Sarsa。 它由一个线性字符串组成
采样的动作和状态，就像在n步骤Sarsa中，除了它的最后一个元素
一个分支在所有行动可能性加权，一如既往，按其概率
π。 该算法可以通过与n步骤Sarsa（上述）相同的方程来描述，
除非定义为n步返回

 任意初始化Q（s，a），∀s∈S，a∈A
将π初始化为相对于Q的ε贪心，或者固定的给定策略
参数：步长α∈（0,1]，小ε> 0，正整数n
所有存储和访问操作（对于S t，A t和R t）可以取其索引模数
重复（每个剧集）：
初始化并存储S 0 6 =终端
选择并存储动作A 0〜π（·| S 0）
T←∞
对于t = 0,1,2，...：
| 如果t <T，则：
| 采取行动
| 观察并将下一个奖励存储为R t + 1，将下一个奖励存储为S t + 1
| 如果S t + 1是终端，则：
| T←t + 1
| 其他：
| 选择并存储动作A t + 1〜π（·| S t + 1）
| τ←t-n + 1（τ是其估计正被更新的时间）
| 如果τ≥0：
| G←
P min（τ+ n，T）
i =τ+ 1
γi-τ-1 R i
| 如果τ+ n <T，则G←G +γn Q（Sτ+ n，Aτ+ n）（G（n）
τ
）
| Q（Sτ，Aτ）←Q（Sτ，Aτ）+α[G-Q（Sτ，Aτ）]
| 如果π是l


图7.4：由于使用n步，策略学习的加速的Gridworld示例
方法。 第一个面板显示代理在单个剧集中所采用的路径，结束于
高奖励的位置，由G.标记。在该示例中，值全部最初为0，
并且所有的奖励都为零，除了在G的积极奖励。另外两个的箭头
面板显示作为这一路径的结果通过一步和哪些行动值被加强
n步骤萨尔萨方法。 一步法仅加强序列的最后一个动作
导致高奖励的行动，而n步法加强了最后的n
动作的序列，使得从这一集中学到更多。


7.3通过重要性抽样的n阶非政策学习
回想一下，非政策学习正在学习一个政策的价值函数π
遵循另一个策略，μ。 通常，π是当前行动的贪婪政策 - 
价值函数估计，μ是一个更具探索性的政策，可能是贪婪的。 在
为了使用来自μ的数据，我们必须考虑到之间的差异
两个策略，使用他们采取行动的相对概率
（见第5.5节）。 在n步法中，返回构建在n个步骤上，所以我们是
感兴趣的只是那些n动作的相对概率。 例如，做
n步骤TD的非策略版本，1对时间t的更新（实际上是在时间t做出的
t + n）可以简单地由ρt + n加权

其中ρt + n
t
，称为重要性抽样率，是相对概率以下
将n个动作从A t变为A t + n-1的两个策略（参见公式5.3）：

例如，如果任何一个动作永远不会被π所采取（即，π（A k | S k）= 0）
则n步返回应该被赋予零权重并且被完全忽略。 上的
另一方面，如果偶然采取一个行动，π将采取更大的
概率比μ，那么这将增加否则将是的权重
给予回报。 这是有道理的，因为该行为是π的特征
（因此我们想要了解它），但很少被μ选择，因此很少
出现在数据中。 为了弥补这一点，我们必须加重它，当它
发生。 请注意，如果两个策略实际上是相同的（on-policy情况），那么
重要性采样率总是1.因此我们的新更新（7.7）概括
并可以完全替代我们早期的n步TD更新。 同样，我们以前的
n步骤Sarsa更新可以完全由其常规的策略外形替换：

注意，重要性采样率这里比n步长TD晚一步
（以上）。 这是因为这里我们更新一个状态 - 动作对。 我们没有
关心我们选择行动的可能性; 现在我们已经选择了我们想要的
以完全了解发生了什么，只有后续的重要性抽样
动作。 完整算法的伪代码显示在下一页的框中。
n-step Expected Sarsa的非策略版本将使用相同的更新
以上为萨尔萨，除了重要性抽样率将有额外
一个较少的因素。 也就是说，上述等式将使用ρt + n-1
t + 1
而不是ρt + n
t + 1，


1本节介绍的算法是非政策性n步TD的最简单形式。 那里
可能是基于第5章中开发的想法的其他，包括具有加权重要性的那些
抽样和每报酬重要性抽样。 这是进一步研究的好主题。

当然它将使用预期的Sarsa版本的n步返回（7.6）。
这是因为在期望的萨尔萨中所有可能的行动都被考虑进去
最后状态;实际采取的没有效果，不必纠正。
我们在本节和第5章中使用的重要性抽样
允许非政策学习，但是以增加更新的方差为代价。
高方差迫使我们使用一个小的步长参数，导致缓慢
学习。非政策培训可能不可避免地比政策上的慢
培训 - 毕竟，数据与你试图学习的内容不太相关。然而，
这也许是真的，我们在这里提出的方法可以改进
上。一种可能性是快速地将步长适应于观察到的方差，如下
Autostep方法（Mahmood等人，2012）。另一种有前景的方法是
不变更新Karampatziakis和Langford（2010）。使用技巧
Mahmood和Sutton（2015）也可能是解决方案的一部分。在下一节
我们考虑不使用重要性抽样的非政策学习方法。

7.4没有重要性的非政策学习抽样：
n步树备份算法
3步
树备份
是否可能没有重要性抽样的非政策学习？ Q-
学习和预期Sarsa方法从第6章证明它可以
在一步的情况下做，但是有一个相应的多步骤
算法？在本节中，我们给出了这样一个n步法，称为
树备份算法。这个想法是由3步树 - 
备份图如右图所示。此备份是一种交替混合
的样本转换 - 从每个动作到随后的状态
完全备份 - 从每个州，我们考虑所有可能的行动，他们
在π下发生的概率，以及它们的动作值。那个行动
实际采取的是特别对待。注意，对于这个动作我们
有一个样本下一个状态，因此我们不需要引导
使用该动作的值;相反我们可以继续下一个状态
到其行动价值。这可以继续直到n的步骤
n步树备份耗尽。每个状态到箭头是
通过在目标下采取行动的概率加权
政策。所采取的一个行动也被其存在的概率加权
但该加权不是应用于其动作值，而是应用于整个树
下面它。也就是说，在第二分支处，未选择的动作具有它们的值
由这个动作概率乘以它们自己被采用的概率（all
在π下）。第三个分支处的操作值（所示备份中的最后一个）
每个都由三个概率加权，如果备份已经更长，则依此类推。
为了清楚和紧凑地写出方程，定义一些新的标量是有用的
随机变量。首先，我们定义目标策略下的预期动作值：


其中我们使用约定，零因子的乘积被定义为1
target然后与来自n-step Sarsa的通常的动作值更新规则一起使用：

而所有其他状态的值保持不变，Q t + n（s，a）= Q t + n-1（s，a）
使得s 6 = S t或a 6 = A t。 此算法的伪代码显示在框中。


任意初始化Q（s，a），∀s∈S，a∈A
将π初始化为相对于Q的ε贪婪，或作为固定给定策略
参数：步长α∈（0,1]，小ε> 0，正整数n
所有存储和访问操作都可以取其索引模数
重复（每个剧集）：
初始化并存储S 0 6 =终端
选择并存储动作A 0〜π（·| S 0）
将Q（S 0，A 0）存储为Q 0
T←∞
对于t = 0,1,2，...：
|如果t <T：
|采取行动
|观察下一个奖励R;观察并存储下一个状态为S t + 1
|如果S t + 1是端子：
| T←t + 1
|将R-Q t存储为δt
|其他：
|存储R +γ
P
aπ（a | S t + 1）Q（S t + 1，a）-Q t
作为δt
|任意选择并将操作存储为A t + 1
|将Q（S t + 1，A t + 1）存储为Q t + 1
|将π（A t + 1 | S t + 1）存储为πt + 1
| τ←t-n + 1（τ是其估计正被更新的时间）
|如果τ≥0：
| E←1
| G←Qτ
|对于k =τ，...，min（τ+ n-1，T-1）：
| G←G +Eδk
| E←γEπk + 1
| Q（Sτ，Aτ）←Q（Sτ，Aτ）+α[G-Q（Sτ，Aτ）]
|如果正在学习π，则确保π（a | Sτ）为ε贪婪wrt Q（Sτ，·）
直到τ= T-1

* 7.5
统一算法：n步Q（σ）
到目前为止，在本章中，我们已经考虑了三种不同的动作值备份，
响应前面三个备份图，如图7.5所示。萨尔萨的步骤
所有采样的转换，树备份算法具有所有状态到动作的转换
完全分支而不采样，并且n步骤预期的Sarsa备份具有所有sam-
除了最后一个国家到行动国家，它是完全分支的
期望值。这些算法在什么程度上可以统一？
图7.5中的第四个备份图提出了一个统一的想法。
这是一个想法，人们可以决定一步一步的基础，无论是否想
将操作作为样本，如在Sarsa中，或考虑对所有操作的期望
而是在树备份中。然后，如果选择总是采样，则将获得
Sarsa，而如果选择不采样，则将得到树备份算法。
预期Sarsa将是这样的情况，其中人们选择对除了的所有步骤进行采样
最后一个。当然，还有许多其他的可能性，如建议的
最后一张图在图中。为了进一步增加可能性，我们可以考虑
采样和期望之间的连续变化。令σt∈[0,1]表示
在步骤t上的采样度，其中σ= 1表示完全采样，σ= 0表示
一个没有抽样的纯粹期望。随机变量σt可以被设置为a
在时间t处的状态，动作或状态 - 动作对的函数。我们称之为提议
新算法n阶Q（σ）。





